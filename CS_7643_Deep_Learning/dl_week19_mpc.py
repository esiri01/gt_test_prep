# question_gatech_MDP_1_5 = {
#     'question': (
#         'Which of the following is TRUE about GANs and VAEs?'
#     ),
#     'options_list': [
#         'A. Both GANs and VAEs use a discriminator to generate realistic samples.',
#         'B. The generator in a GAN optimizes a reconstruction loss.',
#         'C. VAEs maximize a lower bound on the log-likelihood of the data.',
#         'D. GANs and VAEs require labeled data to train effectively.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'VAEs optimize the Evidence Lower Bound (ELBO) to maximize the likelihood of the data. A is incorrect because only GANs use a discriminator. B is false because GANs do not optimize reconstruction losses. D is incorrect as both GANs and VAEs work with unlabeled data.'
#     ),
#     'chapter_information': 'gatech MDP lecture'
# }


# question_GAN_VAE_1 = {
#     'question': (
#         'Which of the following best describes the role of the discriminator in a Generative Adversarial Network (GAN)?'
#     ),
#     'options_list': [
#         'A. It generates realistic samples from the learned probability distribution.',
#         'B. It evaluates how well the generated samples match the true data distribution.',
#         'C. It predicts the conditional probability of a label given the input.',
#         'D. It directly optimizes the joint probability distribution over the input space.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The discriminator in a GAN is trained to distinguish between real samples from the dataset and fake samples generated by the generator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_2 = {
#     'question': (
#         'Which of the following loss functions is used to train the generator in a standard GAN?'
#     ),
#     'options_list': [
#         'A. Mean Squared Error (MSE) loss.',
#         'B. Negative log-likelihood of the generated data.',
#         'C. Cross-entropy loss to maximize the probability of fooling the discriminator.',
#         'D. Variational lower bound on the log-likelihood of the data.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'The generator\'s objective in a GAN is to generate samples that maximize the discriminator\'s classification error, '
#         'which is commonly achieved using cross-entropy loss.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_3 = {
#     'question': (
#         'Which of the following statements is true about the training process of Variational Autoencoders (VAEs)?'
#     ),
#     'options_list': [
#         'A. VAEs optimize a max-min objective to balance generation and discrimination.',
#         'B. VAEs maximize a variational lower bound that includes reconstruction and regularization terms.',
#         'C. VAEs learn an explicit conditional distribution for each input-output pair.',
#         'D. VAEs use adversarial training to ensure generated samples match the true distribution.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'VAEs optimize a variational lower bound that combines reconstruction loss (ensuring generated samples resemble the input) '
#         'and a KL-divergence regularization term (ensuring the latent space follows a prior distribution).'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_4 = {
#     'question': (
#         'How do GANs differ from VAEs in their approach to generative modeling?'
#     ),
#     'options_list': [
#         'A. GANs use a probabilistic framework, while VAEs use adversarial training.',
#         'B. GANs generate samples directly, while VAEs explicitly model the joint distribution.',
#         'C. GANs train with an adversarial loss, while VAEs optimize a variational lower bound.',
#         'D. GANs require a prior distribution in the latent space, while VAEs do not.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'GANs rely on adversarial training between the generator and discriminator, while VAEs optimize a variational lower bound '
#         'to reconstruct inputs and regularize the latent space.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_5 = {
#     'question': (
#         'What is a key limitation of GANs compared to VAEs?'
#     ),
#     'options_list': [
#         'A. GANs cannot generate high-dimensional data.',
#         'B. GANs lack an explicit likelihood function and cannot evaluate the quality of samples probabilistically.',
#         'C. GANs require labeled data to train effectively.',
#         'D. GANs enforce a strict Gaussian prior on the latent space.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'GANs are implicit density models and do not explicitly model the likelihood of the data, making it difficult to evaluate sample quality mathematically.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_6 = {
#     'question': (
#         'True or False: GANs model an explicit probability density function over the data distribution.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'GANs are implicit density models, which do not explicitly define a probability density function but can generate samples from the learned distribution.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_7 = {
#     'question': (
#         'True or False: In a VAE, the latent space is regularized to follow a prior distribution, such as a standard Gaussian.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'VAEs use a KL-divergence term in their loss function to ensure the latent space approximates a prior distribution, commonly a standard Gaussian.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_8 = {
#     'question': (
#         'True or False: The generator in a GAN directly optimizes the KL-divergence between the generated and true distributions.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'GANs optimize an adversarial loss rather than explicitly minimizing KL-divergence.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_9 = {
#     'question': (
#         'True or False: VAEs are capable of both generating new samples and reconstructing inputs using their encoder-decoder structure.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'VAEs can reconstruct inputs using the encoder-decoder structure and generate new samples by sampling from the latent space.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_10 = {
#     'question': (
#         'True or False: GANs can suffer from mode collapse, where the generator only produces a limited variety of outputs.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'Mode collapse is a common issue in GANs, where the generator learns to produce a few specific outputs that fool the discriminator effectively.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_11 = {
#     'question': (
#         'What is the main difference between Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)?'
#     ),
#     'options_list': [
#         'A. GANs explicitly model the density function $p(x)$, while VAEs do not.',
#         'B. GANs generate samples through adversarial training, while VAEs learn a probabilistic latent representation of the data.',
#         'C. GANs use reconstruction loss, while VAEs use adversarial loss.',
#         'D. GANs are limited to images, while VAEs can handle any data type.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'GANs focus on adversarial training to generate realistic samples, while VAEs learn a probabilistic representation in the latent space '
#         'and optimize a reconstruction loss.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_12 = {
#     'question': (
#         'In GANs, what is the role of the discriminator?'
#     ),
#     'options_list': [
#         'A. To generate realistic samples from random noise.',
#         'B. To distinguish between real and fake samples generated by the generator.',
#         'C. To optimize the generator\'s loss function.',
#         'D. To predict the class labels of input data.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The discriminator learns to distinguish real data from fake data generated by the generator, providing feedback to improve the generator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_13 = {
#     'question': (
#         'What is the primary objective of the generator in GANs?'
#     ),
#     'options_list': [
#         'A. To maximize the discriminator\'s ability to classify real and fake data.',
#         'B. To minimize the difference between the generated and real data in feature space.',
#         'C. To fool the discriminator into classifying generated data as real.',
#         'D. To reconstruct input data as accurately as possible.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'The generator\'s goal is to generate data so realistic that the discriminator cannot distinguish it from real data.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_14 = {
#     'question': (
#         'What loss function does the generator minimize in the modified GAN training objective?'
#     ),
#     'options_list': [
#         'A. $\\log(1−D(G(z)))$',
#         'B. $\\log(D(G(z)))$',
#         'C. $−\\log(D(G(z)))$',
#         'D. $D(G(z))$'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The modified objective maximizes $\\log(D(G(z)))$, providing more stable gradients when training the generator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_15 = {
#     'question': (
#         'What is "mode collapse" in GANs?'
#     ),
#     'options_list': [
#         'A. When the discriminator outperforms the generator consistently.',
#         'B. When the generator memorizes and outputs the training samples.',
#         'C. When the generator fails to capture the full diversity of the data distribution.',
#         'D. When the discriminator cannot distinguish between real and fake data.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Mode collapse occurs when the generator focuses on a subset of the data distribution, failing to produce diverse outputs.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_16 = {
#     'question': (
#         'True or False: GANs explicitly learn a probability density function $p(x)$.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'GANs implicitly learn to generate samples from $p(x)$ but do not explicitly model the density function.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_17 = {
#     'question': (
#         'True or False: Variational Autoencoders (VAEs) optimize a reconstruction loss and a regularization term that enforces a Gaussian distribution in the latent space.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'VAEs combine a reconstruction loss with a KL divergence regularization term to ensure a structured latent space.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_18 = {
#     'question': (
#         'True or False: GANs rely on two neural networks: a generator and a discriminator, which compete in a minimax optimization game.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'GANs use adversarial training, where the generator tries to fool the discriminator, and the discriminator tries to distinguish real from fake data.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_19 = {
#     'question': (
#         'True or False: Adding noise to real images during GAN training can improve stability and reduce mode collapse.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'Adding noise is a regularization technique that helps stabilize training and prevents overfitting by the discriminator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_20 = {
#     'question': (
#         'True or False: The generator in a GAN can function independently of the discriminator once training is complete.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'After training, the generator can produce samples without needing the discriminator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }


question_gatech_MDP_1_5 = {
    'question': (
        'Which of the following is TRUE about GANs and VAEs? (Probability of appearing on test: 0.745)'
    ),
    'options_list': [
        'A. Both GANs and VAEs use a discriminator to generate realistic samples.',
        'B. The generator in a GAN optimizes a reconstruction loss.',
        'C. VAEs maximize a lower bound on the log-likelihood of the data.',
        'D. GANs and VAEs require labeled data to train effectively.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'VAEs optimize the Evidence Lower Bound (ELBO) to maximize the likelihood of the data. A is incorrect because only GANs use a discriminator. '
        'B is false because GANs do not optimize reconstruction losses. D is incorrect as both GANs and VAEs work with unlabeled data.'
    ),
    'chapter_information': 'gatech MDP lecture'
}

question_GAN_VAE_1 = {
    'question': (
        'Which of the following best describes the role of the discriminator in a Generative Adversarial Network (GAN)? (Probability of appearing on test: 0.743)'
    ),
    'options_list': [
        'A. It generates realistic samples from the learned probability distribution.',
        'B. It evaluates how well the generated samples match the true data distribution.',
        'C. It predicts the conditional probability of a label given the input.',
        'D. It directly optimizes the joint probability distribution over the input space.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The discriminator in a GAN is trained to distinguish between real samples from the dataset and fake samples generated by the generator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_2 = {
    'question': (
        'Which of the following loss functions is used to train the generator in a standard GAN? (Probability of appearing on test: 0.966)'
    ),
    'options_list': [
        'A. Mean Squared Error (MSE) loss.',
        'B. Negative log-likelihood of the generated data.',
        'C. Cross-entropy loss to maximize the probability of fooling the discriminator.',
        'D. Variational lower bound on the log-likelihood of the data.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'The generator\'s objective in a GAN is to generate samples that maximize the discriminator\'s classification error, '
        'which is commonly achieved using cross-entropy loss.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_3 = {
    'question': (
        'Which of the following statements is true about the training process of Variational Autoencoders (VAEs)? (Probability of appearing on test: 0.934)'
    ),
    'options_list': [
        'A. VAEs optimize a max-min objective to balance generation and discrimination.',
        'B. VAEs maximize a variational lower bound that includes reconstruction and regularization terms.',
        'C. VAEs learn an explicit conditional distribution for each input-output pair.',
        'D. VAEs use adversarial training to ensure generated samples match the true distribution.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'VAEs optimize a variational lower bound that combines reconstruction loss (ensuring generated samples resemble the input) '
        'and a KL-divergence regularization term (ensuring the latent space follows a prior distribution).'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_4 = {
    'question': (
        'How do GANs differ from VAEs in their approach to generative modeling? (Probability of appearing on test: 0.779)'
    ),
    'options_list': [
        'A. GANs use a probabilistic framework, while VAEs use adversarial training.',
        'B. GANs generate samples directly, while VAEs explicitly model the joint distribution.',
        'C. GANs train with an adversarial loss, while VAEs optimize a variational lower bound.',
        'D. GANs require a prior distribution in the latent space, while VAEs do not.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'GANs rely on adversarial training between the generator and discriminator, while VAEs optimize a variational lower bound '
        'to reconstruct inputs and regularize the latent space.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_5 = {
    'question': (
        'What is a key limitation of GANs compared to VAEs? (Probability of appearing on test: 0.949)'
    ),
    'options_list': [
        'A. GANs cannot generate high-dimensional data.',
        'B. GANs lack an explicit likelihood function and cannot evaluate the quality of samples probabilistically.',
        'C. GANs require labeled data to train effectively.',
        'D. GANs enforce a strict Gaussian prior on the latent space.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'GANs are implicit density models and do not explicitly model the likelihood of the data, making it difficult to evaluate sample quality mathematically.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_6 = {
    'question': (
        'True or False: GANs model an explicit probability density function over the data distribution. (Probability of appearing on test: 0.902)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'GANs are implicit density models, which do not explicitly define a probability density function but can generate samples from the learned distribution.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_7 = {
    'question': (
        'True or False: In a VAE, the latent space is regularized to follow a prior distribution, such as a standard Gaussian. (Probability of appearing on test: 0.862)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'VAEs use a KL-divergence term in their loss function to ensure the latent space approximates a prior distribution, commonly a standard Gaussian.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_8 = {
    'question': (
        'True or False: The generator in a GAN directly optimizes the KL-divergence between the generated and true distributions. (Probability of appearing on test: 0.775)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'GANs optimize an adversarial loss rather than explicitly minimizing KL-divergence.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_9 = {
    'question': (
        'True or False: VAEs are capable of both generating new samples and reconstructing inputs using their encoder-decoder structure. (Probability of appearing on test: 0.850)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'VAEs can reconstruct inputs using the encoder-decoder structure and generate new samples by sampling from the latent space.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_10 = {
    'question': (
        'True or False: GANs can suffer from mode collapse, where the generator only produces a limited variety of outputs. (Probability of appearing on test: 0.836)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'Mode collapse is a common issue in GANs, where the generator learns to produce a few specific outputs that fool the discriminator effectively.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_11 = {
    'question': (
        'What is the main difference between Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)? (Probability of appearing on test: 0.846)'
    ),
    'options_list': [
        'A. GANs explicitly model the density function $p(x)$, while VAEs do not.',
        'B. GANs generate samples through adversarial training, while VAEs learn a probabilistic latent representation of the data.',
        'C. GANs use reconstruction loss, while VAEs use adversarial loss.',
        'D. GANs are limited to images, while VAEs can handle any data type.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'GANs focus on adversarial training to generate realistic samples, while VAEs learn a probabilistic representation in the latent space '
        'and optimize a reconstruction loss.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_12 = {
    'question': (
        'In GANs, what is the role of the discriminator? (Probability of appearing on test: 0.462)'
    ),
    'options_list': [
        'A. To generate realistic samples from random noise.',
        'B. To distinguish between real and fake samples generated by the generator.',
        'C. To optimize the generator\'s loss function.',
        'D. To predict the class labels of input data.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The discriminator learns to distinguish real data from fake data generated by the generator, providing feedback to improve the generator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_13 = {
    'question': (
        'What is the primary objective of the generator in GANs? (Probability of appearing on test: 0.916)'
    ),
    'options_list': [
        'A. To maximize the discriminator\'s ability to classify real and fake data.',
        'B. To minimize the difference between the generated and real data in feature space.',
        'C. To fool the discriminator into classifying generated data as real.',
        'D. To reconstruct input data as accurately as possible.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'The generator\'s goal is to generate data so realistic that the discriminator cannot distinguish it from real data.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_14 = {
    'question': (
        'What loss function does the generator minimize in the modified GAN training objective? (Probability of appearing on test: 0.848)'
    ),
    'options_list': [
        'A. $\\log(1−D(G(z)))$',
        'B. $\\log(D(G(z)))$',
        'C. $−\\log(D(G(z)))$',
        'D. $D(G(z))$'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The modified objective maximizes $\\log(D(G(z)))$, providing more stable gradients when training the generator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_15 = {
    'question': (
        'What is "mode collapse" in GANs? (Probability of appearing on test: 0.535)'
    ),
    'options_list': [
        'A. When the discriminator outperforms the generator consistently.',
        'B. When the generator memorizes and outputs the training samples.',
        'C. When the generator fails to capture the full diversity of the data distribution.',
        'D. When the discriminator cannot distinguish between real and fake data.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Mode collapse occurs when the generator focuses on a subset of the data distribution, failing to produce diverse outputs.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_16 = {
    'question': (
        'True or False: GANs explicitly learn a probability density function $p(x)$. (Probability of appearing on test: 0.890)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'GANs implicitly learn to generate samples from $p(x)$ but do not explicitly model the density function.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_17 = {
    'question': (
        'True or False: Variational Autoencoders (VAEs) optimize a reconstruction loss and a regularization term that enforces a Gaussian distribution in the latent space. (Probability of appearing on test: 0.970)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'VAEs combine a reconstruction loss with a KL divergence regularization term to ensure a structured latent space.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_18 = {
    'question': (
        'True or False: GANs rely on two neural networks: a generator and a discriminator, which compete in a minimax optimization game. (Probability of appearing on test: 0.790)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'GANs use adversarial training, where the generator tries to fool the discriminator, and the discriminator tries to distinguish real from fake data.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_19 = {
    'question': (
        'True or False: Adding noise to real images during GAN training can improve stability and reduce mode collapse. (Probability of appearing on test: 0.881)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'Adding noise is a regularization technique that helps stabilize training and prevents overfitting by the discriminator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_20 = {
    'question': (
        'True or False: The generator in a GAN can function independently of the discriminator once training is complete. (Probability of appearing on test: 0.819)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'After training, the generator can produce samples without needing the discriminator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

#########################

# question_GAN_VAE_21 = {
#     'question': (
#         'In a Variational Autoencoder (VAE), which of the following statements best describes the role of the encoder and decoder networks?'
#     ),
#     'options_list': [
#         'A. The encoder generates samples from a prior distribution, and the decoder reconstructs the input data.',
#         'B. The encoder maps input data to latent variables, and the decoder reconstructs the input data from the latent variables.',
#         'C. The encoder and decoder are trained separately; the encoder generates labels for supervised learning.',
#         'D. The encoder reconstructs the input data, and the decoder maps the reconstructed data back to latent variables.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'In a VAE, the encoder maps input data $x$ to a latent variable $z$ by estimating the parameters of the approximate posterior distribution '
#         '$q_\\phi(z \\mid x)$. The decoder then reconstructs the input data from the latent variables by modeling $p_\\theta(x \\mid z)$.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_22 = {
#     'question': (
#         'What is the primary objective function used to train a VAE?'
#     ),
#     'options_list': [
#         'A. Maximizing the log-likelihood of the data.',
#         'B. Minimizing the reconstruction error only.',
#         'C. Maximizing the evidence lower bound (ELBO) on the data likelihood.',
#         'D. Minimizing the total variation distance between the data distribution and the model distribution.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'The primary objective in training a VAE is to maximize the evidence lower bound (ELBO) on the data likelihood. '
#         'This involves maximizing the expected log-likelihood of the data given the latent variables and minimizing the KL divergence '
#         'between the approximate posterior and the prior distribution.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_23 = {
#     'question': (
#         'In training a VAE, the Kullback-Leibler (KL) divergence term in the loss function serves what purpose?'
#     ),
#     'options_list': [
#         'A. It measures the difference between the reconstructed data and the input data.',
#         'B. It enforces the approximate posterior distribution to be close to the prior distribution.',
#         'C. It ensures that the encoder and decoder networks have similar weights.',
#         'D. It penalizes the model for overfitting to the training data.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The KL divergence term in the VAE loss function enforces the approximate posterior distribution $q_\\phi(z \\mid x)$ to be close '
#         'to the prior distribution $p(z)$. This regularization encourages the latent space to follow the prior distribution, facilitating sampling and ensuring meaningful latent representations.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_24 = {
#     'question': (
#         'In a Generative Adversarial Network (GAN), what are the roles of the generator and discriminator?'
#     ),
#     'options_list': [
#         'A. The generator outputs labels for the data, and the discriminator classifies the data.',
#         'B. The generator produces fake data samples, and the discriminator distinguishes between real and fake samples.',
#         'C. The generator reconstructs input data, and the discriminator estimates the likelihood of the data.',
#         'D. Both the generator and discriminator generate samples from the latent space.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'In a GAN, the generator produces fake data samples by mapping random noise from the latent space to the data space. '
#         'The discriminator aims to distinguish between real data samples and the fake samples produced by the generator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_25 = {
#     'question': (
#         'Which of the following best describes the training objective of a GAN?'
#     ),
#     'options_list': [
#         'A. The generator and discriminator minimize their respective losses independently.',
#         'B. The generator tries to minimize the Jensen-Shannon divergence between the data and model distributions.',
#         'C. The generator maximizes the log-likelihood of the data.',
#         'D. The generator minimizes the reconstruction error between the input data and output data.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The training objective of a GAN is for the generator to produce data that the discriminator cannot distinguish from real data. '
#         'This can be interpreted as the generator trying to minimize the Jensen-Shannon divergence between the real data distribution and the generated data distribution through the adversarial training process.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_26 = {
#     'question': (
#         'What is a key difference between VAEs and GANs in terms of their approach to generative modeling?'
#     ),
#     'options_list': [
#         'A. VAEs are implicit density models, while GANs are explicit density models.',
#         'B. VAEs use a deterministic mapping from latent variables to data, while GANs use stochastic mapping.',
#         'C. VAEs aim to approximate the data likelihood directly, while GANs use an adversarial approach without an explicit likelihood.',
#         'D. VAEs require a discriminator network, while GANs do not.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'VAEs aim to approximate the data likelihood directly by maximizing the ELBO, involving explicit probability densities. '
#         'GANs, on the other hand, use an adversarial approach to generate data without explicitly defining or computing the data likelihood.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_27 = {
#     'question': (
#         'In the context of VAEs, what is the "reparameterization trick"?'
#     ),
#     'options_list': [
#         'A. A method to approximate the KL divergence term in the loss function.',
#         'B. A technique to sample from the posterior distribution without gradients flowing through the sampling operation.',
#         'C. A way to enforce a prior distribution on the latent variables.',
#         'D. A method to increase the capacity of the encoder network.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The reparameterization trick allows gradients to flow through the sampling operation by expressing the sampled latent variable '
#         '$z$ as a deterministic function of $\\epsilon$ (random noise) and the parameters $\\mu$ and $\\sigma$: $z = \\mu + \\sigma \\cdot \\epsilon$. '
#         'This makes the sampling process differentiable with respect to the encoder parameters.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_28 = {
#     'question': (
#         'In GANs, which of the following is a common problem that arises during training?'
#     ),
#     'options_list': [
#         'A. Mode collapse, where the generator produces a limited variety of outputs.',
#         'B. Vanishing gradients, leading to slow convergence of the generator.',
#         'C. Overfitting, due to the generator memorizing the training data.',
#         'D. Lack of a latent space representation, making interpolation impossible.'
#     ],
#     'correct_answer': 'A',
#     'explanation': (
#         'A common problem in GANs is mode collapse, where the generator learns to produce a limited variety of outputs, failing to capture the diversity '
#         'of the real data distribution. This happens when the generator finds a narrow range of outputs that consistently fool the discriminator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_29 = {
#     'question': (
#         'Which loss function is typically minimized by the discriminator in a standard GAN?'
#     ),
#     'options_list': [
#         'A. Cross-entropy loss between the predicted and true labels of real and fake data.',
#         'B. Mean squared error between the real data and generated data.',
#         'C. The Kullback-Leibler divergence between the real and generated data distributions.',
#         'D. The reconstruction loss between the input and output data.'
#     ],
#     'correct_answer': 'A',
#     'explanation': (
#         'The discriminator in a standard GAN minimizes the cross-entropy loss (also known as the binary classification loss) between its predictions and the true labels (real or fake) of the data samples.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_VAE_30 = {
#     'question': (
#         'True or False: VAEs generally produce sharper images than GANs because they maximize the data likelihood directly.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'This statement is false. GANs generally produce sharper images than VAEs. VAEs often produce blurrier images because they optimize for the likelihood, '
#         'which can lead to averaging effects in the generated samples. GANs, through their adversarial training, tend to produce sharper and more realistic images.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# # Multiple-Choice Questions
# question_GAN_31 = {
#     'question': (
#         'In the context of GANs, what is the primary role of the generator network?'
#     ),
#     'options_list': [
#         'A. To classify input data as real or fake.',
#         'B. To map noise variables to data space, generating new data samples.',
#         'C. To estimate the probability density function of the training data.',
#         'D. To encode input data into a lower-dimensional representation.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The generator network maps noise from the latent space to the data space, creating new data samples that resemble the training data.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_32 = {
#     'question': (
#         'What does the discriminator network aim to achieve in a GAN?'
#     ),
#     'options_list': [
#         'A. It tries to reconstruct input data from latent variables.',
#         'B. It generates new data samples to fool the generator.',
#         'C. It distinguishes between real data and data generated by the generator.',
#         'D. It minimizes the Kullback-Leibler divergence between data distributions.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'The discriminator is trained to distinguish between real samples from the dataset and fake samples generated by the generator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_33 = {
#     'question': (
#         'Which of the following best describes the training objective of a standard GAN?'
#     ),
#     'options_list': [
#         'A. A minimax game where the generator minimizes and the discriminator maximizes the same loss function.',
#         'B. A cooperative game where both networks aim to minimize reconstruction error.',
#         'C. A sequential game where the generator is trained after the discriminator converges.',
#         'D. A supervised learning task where both networks are trained on labeled data.'
#     ],
#     'correct_answer': 'A',
#     'explanation': (
#         'The training process in GANs is a minimax game where the generator and discriminator compete using the same adversarial loss function.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_34 = {
#     'question': (
#         'What is mode collapse in the context of GANs?'
#     ),
#     'options_list': [
#         'A. When the discriminator overpowers the generator, leading to vanishing gradients.',
#         'B. When the generator produces a limited variety of outputs, ignoring parts of the data distribution.',
#         'C. When both the generator and discriminator fail to converge during training.',
#         'D. When the GAN model overfits the training data, memorizing exact samples.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'Mode collapse occurs when the generator only produces outputs from a narrow range of the data distribution, reducing diversity.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_35 = {
#     'question': (
#         'Which of the following is a common technique to mitigate mode collapse in GANs?'
#     ),
#     'options_list': [
#         'A. Using higher learning rates for the discriminator.',
#         'B. Implementing batch normalization only in the generator.',
#         'C. Employing feature matching and minibatch discrimination.',
#         'D. Reducing the capacity of the generator network.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Feature matching and minibatch discrimination help prevent mode collapse by encouraging diversity in the generator\'s outputs.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# # True/False Questions
# question_GAN_36 = {
#     'question': (
#         'True or False: GANs always minimize the Kullback-Leibler divergence between the data distribution and the model distribution.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'False',
#     'explanation': (
#         'GANs do not explicitly minimize the Kullback-Leibler divergence. Instead, they use adversarial loss to align the distributions implicitly.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_37 = {
#     'question': (
#         'True or False: In GANs, the generator network directly accesses the training data during the update.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'False',
#     'explanation': (
#         'The generator does not access training data directly; it generates samples from random noise to compete with the discriminator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_38 = {
#     'question': (
#         'True or False: Batch normalization is used in GANs to stabilize training by normalizing inputs to each layer.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'True',
#     'explanation': (
#         'Batch normalization is a common technique to stabilize GAN training by reducing internal covariate shifts.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_39 = {
#     'question': (
#         'True or False: Mode collapse occurs when the discriminator fails to learn and the generator overfits the training data.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'False',
#     'explanation': (
#         'Mode collapse happens when the generator learns to produce a limited set of outputs that consistently fool the discriminator.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }

# question_GAN_40 = {
#     'question': (
#         'True or False: GANs can be used for semi-supervised learning by modifying the discriminator to output class labels.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'True',
#     'explanation': (
#         'By extending the discriminator to output class probabilities in addition to distinguishing real vs. fake samples, GANs can be adapted for semi-supervised learning.'
#     ),
#     'chapter_information': 'Gatech Lectures - GANs and VAEs'
# }


# # Multiple-Choice Questions
# question_VAE_41 = {
#     'question': (
#         'Which of the following statements best describes the role of the reparameterization trick in Variational Autoencoders (VAEs)?'
#     ),
#     'options_list': [
#         'A. It approximates the intractable KL divergence term in the VAE objective.',
#         'B. It ensures the encoder produces deterministic latent variables for accurate sampling.',
#         'C. It moves the sampling operation to a differentiable layer, enabling backpropagation.',
#         'D. It replaces the Gaussian prior $P(z)$ with a learned prior to improve regularization.',
#         'E. It eliminates the need for stochastic gradient descent in training VAEs.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'The reparameterization trick allows gradients to flow through the sampling process by reformulating $z$ as a deterministic function '
#         'of the mean, variance, and random noise.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }

# question_VAE_42 = {
#     'question': (
#         'What is the primary purpose of the KL divergence term $D_{KL}(Q(z\\mid X) \\parallel P(z))$ in the VAE loss function?'
#     ),
#     'options_list': [
#         'A. Ensure that latent variable $z$ perfectly reconstructs the input data $X$.',
#         'B. Minimize the distance between the posterior $Q(z\\mid X)$ and the prior $P(z)$.',
#         'C. Penalize high-dimensional latent representations to encourage sparsity.',
#         'D. Maximize the likelihood of $P(X\\mid z)$ under the decoder network.',
#         'E. Regularize the encoder to produce independent samples from a uniform distribution.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The KL divergence term enforces that the learned posterior $Q(z\\mid X)$ aligns with the prior $P(z)$, encouraging a smooth latent space '
#         'for better generalization and sampling.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }

# question_VAE_43 = {
#     'question': (
#         'When training a Variational Autoencoder, which of the following challenges is directly addressed by the use of a probabilistic decoder $P(X\\mid z)$?'
#     ),
#     'options_list': [
#         'A. Overfitting due to the high-dimensional latent space.',
#         'B. Ensuring that generated samples resemble the training data distribution.',
#         'C. Handling the intractability of computing $P(X)$ exactly.',
#         'D. Allowing discrete latent variables in the encoder.',
#         'E. Guaranteeing that latent representations are disentangled.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The decoder $P(X\\mid z)$ generates $X$ given a sampled $z$, ensuring that reconstructed or generated data is consistent with the observed data distribution.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }

# question_VAE_44 = {
#     'question': (
#         'Which of the following conditions would most likely result in poor performance of a VAE?'
#     ),
#     'options_list': [
#         'A. Setting the KL divergence weight to zero during training.',
#         'B. Using a Gaussian decoder for continuous data.',
#         'C. Choosing a latent dimension smaller than the intrinsic data dimensionality.',
#         'D. Using a diagonal covariance matrix for $Q(z\\mid X)$.',
#         'E. Training on a dataset with imbalanced classes.'
#     ],
#     'correct_answer': 'A',
#     'explanation': (
#         'Without the KL divergence term, the encoder is not constrained to produce a distribution aligned with the prior, leading to poor sampling and generalization.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }

# question_VAE_45 = {
#     'question': (
#         'Which of the following methods extends VAEs for tasks involving conditional generation (e.g., image completion)?'
#     ),
#     'options_list': [
#         'A. Use of a multivariate Gaussian decoder.',
#         'B. Training with augmented data to improve robustness.',
#         'C. Incorporating a Conditional Variational Autoencoder (CVAE) that conditions both the encoder and decoder on input $X$.',
#         'D. Replacing the standard KL divergence with Wasserstein loss.',
#         'E. Including adversarial training objectives.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Conditional VAEs modify the generative process to explicitly condition on auxiliary inputs $X$, allowing for conditional generation of outputs.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }

# # True/False Questions
# question_VAE_46 = {
#     'question': (
#         'True or False: The encoder in a Variational Autoencoder is trained to output deterministic latent variables $z$.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'False',
#     'explanation': (
#         'The encoder outputs a distribution over $z$ (parameterized by $\\mu$ and $\\sigma$), not deterministic values.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }

# question_VAE_47 = {
#     'question': (
#         'True or False: The reconstruction loss in a VAE objective function is always computed using the $\\ell_2$-norm between the input $X$ and its reconstruction.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'False',
#     'explanation': (
#         'The reconstruction loss depends on the output distribution; for binary data, a Bernoulli likelihood is often used with cross-entropy loss.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }

# question_VAE_48 = {
#     'question': (
#         'True or False: VAEs assume that the posterior $Q(z\\mid X)$ is independent of the prior $P(z)$.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'False',
#     'explanation': (
#         'The KL divergence term explicitly encourages $Q(z\\mid X)$ to align with $P(z)$, thus enforcing dependence between them.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }

# question_VAE_49 = {
#     'question': (
#         'True or False: Sampling $z \\sim N(\\mu, \\sigma^2)$ directly during training makes the VAE training objective non-differentiable.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'True',
#     'explanation': (
#         'Direct sampling is non-differentiable, which is why the reparameterization trick is necessary.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }

# question_VAE_50 = {
#     'question': (
#         'True or False: Increasing the dimensionality of the latent space always improves the generative quality of a VAE.'
#     ),
#     'options_list': ['True', 'False'],
#     'correct_answer': 'False',
#     'explanation': (
#         'Excessive latent dimensions can lead to overfitting, making the KL divergence term ineffective and resulting in poor generalization.'
#     ),
#     'chapter_information': 'Gatech Lectures - Variational Autoencoders'
# }


question_GAN_VAE_21 = {
    'question': (
        'In a Variational Autoencoder (VAE), which of the following statements best describes the role of the encoder and decoder networks? (Probability of appearing on test: 0.348)'
    ),
    'options_list': [
        'A. The encoder generates samples from a prior distribution, and the decoder reconstructs the input data.',
        'B. The encoder maps input data to latent variables, and the decoder reconstructs the input data from the latent variables.',
        'C. The encoder and decoder are trained separately; the encoder generates labels for supervised learning.',
        'D. The encoder reconstructs the input data, and the decoder maps the reconstructed data back to latent variables.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'In a VAE, the encoder maps input data $x$ to a latent variable $z$ by estimating the parameters of the approximate posterior distribution '
        '$q_\\phi(z \\mid x)$. The decoder then reconstructs the input data from the latent variables by modeling $p_\\theta(x \\mid z)$.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_22 = {
    'question': (
        'What is the primary objective function used to train a VAE? (Probability of appearing on test: 0.917)'
    ),
    'options_list': [
        'A. Maximizing the log-likelihood of the data.',
        'B. Minimizing the reconstruction error only.',
        'C. Maximizing the evidence lower bound (ELBO) on the data likelihood.',
        'D. Minimizing the total variation distance between the data distribution and the model distribution.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'The primary objective in training a VAE is to maximize the evidence lower bound (ELBO) on the data likelihood. '
        'This involves maximizing the expected log-likelihood of the data given the latent variables and minimizing the KL divergence '
        'between the approximate posterior and the prior distribution.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_23 = {
    'question': (
        'In training a VAE, the Kullback-Leibler (KL) divergence term in the loss function serves what purpose? (Probability of appearing on test: 0.428)'
    ),
    'options_list': [
        'A. It measures the difference between the reconstructed data and the input data.',
        'B. It enforces the approximate posterior distribution to be close to the prior distribution.',
        'C. It ensures that the encoder and decoder networks have similar weights.',
        'D. It penalizes the model for overfitting to the training data.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The KL divergence term in the VAE loss function enforces the approximate posterior distribution $q_\\phi(z \\mid x)$ to be close '
        'to the prior distribution $p(z)$. This regularization encourages the latent space to follow the prior distribution, facilitating sampling and ensuring meaningful latent representations.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_24 = {
    'question': (
        'In a Generative Adversarial Network (GAN), what are the roles of the generator and discriminator? (Probability of appearing on test: 0.557)'
    ),
    'options_list': [
        'A. The generator outputs labels for the data, and the discriminator classifies the data.',
        'B. The generator produces fake data samples, and the discriminator distinguishes between real and fake samples.',
        'C. The generator reconstructs input data, and the discriminator estimates the likelihood of the data.',
        'D. Both the generator and discriminator generate samples from the latent space.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'In a GAN, the generator produces fake data samples by mapping random noise from the latent space to the data space. '
        'The discriminator aims to distinguish between real data samples and the fake samples produced by the generator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_25 = {
    'question': (
        'Which of the following best describes the training objective of a GAN? (Probability of appearing on test: 0.649)'
    ),
    'options_list': [
        'A. The generator and discriminator minimize their respective losses independently.',
        'B. The generator tries to minimize the Jensen-Shannon divergence between the data and model distributions.',
        'C. The generator maximizes the log-likelihood of the data.',
        'D. The generator minimizes the reconstruction error between the input data and output data.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The training objective of a GAN is for the generator to produce data that the discriminator cannot distinguish from real data. '
        'This can be interpreted as the generator trying to minimize the Jensen-Shannon divergence between the real data distribution and the generated data distribution through the adversarial training process.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_26 = {
    'question': (
        'What is a key difference between VAEs and GANs in terms of their approach to generative modeling? (Probability of appearing on test: 0.893)'
    ),
    'options_list': [
        'A. VAEs are implicit density models, while GANs are explicit density models.',
        'B. VAEs use a deterministic mapping from latent variables to data, while GANs use stochastic mapping.',
        'C. VAEs aim to approximate the data likelihood directly, while GANs use an adversarial approach without an explicit likelihood.',
        'D. VAEs require a discriminator network, while GANs do not.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'VAEs aim to approximate the data likelihood directly by maximizing the ELBO, involving explicit probability densities. '
        'GANs, on the other hand, use an adversarial approach to generate data without explicitly defining or computing the data likelihood.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_27 = {
    'question': (
        'In the context of VAEs, what is the "reparameterization trick"? (Probability of appearing on test: 0.853)'
    ),
    'options_list': [
        'A. A method to approximate the KL divergence term in the loss function.',
        'B. A technique to sample from the posterior distribution without gradients flowing through the sampling operation.',
        'C. A way to enforce a prior distribution on the latent variables.',
        'D. A method to increase the capacity of the encoder network.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The reparameterization trick allows gradients to flow through the sampling operation by expressing the sampled latent variable '
        '$z$ as a deterministic function of $\\epsilon$ (random noise) and the parameters $\\mu$ and $\\sigma$: $z = \\mu + \\sigma \\cdot \\epsilon$. '
        'This makes the sampling process differentiable with respect to the encoder parameters.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_28 = {
    'question': (
        'In GANs, which of the following is a common problem that arises during training? (Probability of appearing on test: 0.904)'
    ),
    'options_list': [
        'A. Mode collapse, where the generator produces a limited variety of outputs.',
        'B. Vanishing gradients, leading to slow convergence of the generator.',
        'C. Overfitting, due to the generator memorizing the training data.',
        'D. Lack of a latent space representation, making interpolation impossible.'
    ],
    'correct_answer': 'A',
    'explanation': (
        'A common problem in GANs is mode collapse, where the generator learns to produce a limited variety of outputs, failing to capture the diversity '
        'of the real data distribution. This happens when the generator finds a narrow range of outputs that consistently fool the discriminator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_29 = {
    'question': (
        'Which loss function is typically minimized by the discriminator in a standard GAN? (Probability of appearing on test: 0.934)'
    ),
    'options_list': [
        'A. Cross-entropy loss between the predicted and true labels of real and fake data.',
        'B. Mean squared error between the real data and generated data.',
        'C. The Kullback-Leibler divergence between the real and generated data distributions.',
        'D. The reconstruction loss between the input and output data.'
    ],
    'correct_answer': 'A',
    'explanation': (
        'The discriminator in a standard GAN minimizes the cross-entropy loss (also known as the binary classification loss) between its predictions and the true labels (real or fake) of the data samples.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_VAE_30 = {
    'question': (
        'True or False: VAEs generally produce sharper images than GANs because they maximize the data likelihood directly. (Probability of appearing on test: 0.891)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'False',
    'explanation': (
        'This statement is false. GANs generally produce sharper images than VAEs. VAEs often produce blurrier images because they optimize for the likelihood, '
        'which can lead to averaging effects in the generated samples. GANs, through their adversarial training, tend to produce sharper and more realistic images.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_31 = {
    'question': (
        'In the context of GANs, what is the primary role of the generator network? (Probability of appearing on test: 0.529)'
    ),
    'options_list': [
        'A. To classify input data as real or fake.',
        'B. To map noise variables to data space, generating new data samples.',
        'C. To estimate the probability density function of the training data.',
        'D. To encode input data into a lower-dimensional representation.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The generator network maps noise from the latent space to the data space, creating new data samples that resemble the training data.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_32 = {
    'question': (
        'What does the discriminator network aim to achieve in a GAN? (Probability of appearing on test: 0.755)'
    ),
    'options_list': [
        'A. It tries to reconstruct input data from latent variables.',
        'B. It generates new data samples to fool the generator.',
        'C. It distinguishes between real data and data generated by the generator.',
        'D. It minimizes the Kullback-Leibler divergence between data distributions.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'The discriminator is trained to distinguish between real samples from the dataset and fake samples generated by the generator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_33 = {
    'question': (
        'Which of the following best describes the training objective of a standard GAN? (Probability of appearing on test: 0.822)'
    ),
    'options_list': [
        'A. A minimax game where the generator minimizes and the discriminator maximizes the same loss function.',
        'B. A cooperative game where both networks aim to minimize reconstruction error.',
        'C. A sequential game where the generator is trained after the discriminator converges.',
        'D. A supervised learning task where both networks are trained on labeled data.'
    ],
    'correct_answer': 'A',
    'explanation': (
        'The training process in GANs is a minimax game where the generator and discriminator compete using the same adversarial loss function.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_34 = {
    'question': (
        'What is mode collapse in the context of GANs? (Probability of appearing on test: 0.618)'
    ),
    'options_list': [
        'A. When the discriminator overpowers the generator, leading to vanishing gradients.',
        'B. When the generator produces a limited variety of outputs, ignoring parts of the data distribution.',
        'C. When both the generator and discriminator fail to converge during training.',
        'D. When the GAN model overfits the training data, memorizing exact samples.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'Mode collapse occurs when the generator only produces outputs from a narrow range of the data distribution, reducing diversity.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_35 = {
    'question': (
        'Which of the following is a common technique to mitigate mode collapse in GANs? (Probability of appearing on test: 0.746)'
    ),
    'options_list': [
        'A. Using higher learning rates for the discriminator.',
        'B. Implementing batch normalization only in the generator.',
        'C. Employing feature matching and minibatch discrimination.',
        'D. Reducing the capacity of the generator network.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Feature matching and minibatch discrimination help prevent mode collapse by encouraging diversity in the generator\'s outputs.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_36 = {
    'question': (
        'True or False: GANs always minimize the Kullback-Leibler divergence between the data distribution and the model distribution. (Probability of appearing on test: 0.924)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'False',
    'explanation': (
        'GANs do not explicitly minimize the Kullback-Leibler divergence. Instead, they use adversarial loss to align the distributions implicitly.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_37 = {
    'question': (
        'True or False: In GANs, the generator network directly accesses the training data during the update. (Probability of appearing on test: 0.659)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'False',
    'explanation': (
        'The generator does not access training data directly; it generates samples from random noise to compete with the discriminator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_38 = {
    'question': (
        'True or False: Batch normalization is used in GANs to stabilize training by normalizing inputs to each layer. (Probability of appearing on test: 0.871)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'True',
    'explanation': (
        'Batch normalization is a common technique to stabilize GAN training by reducing internal covariate shifts.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_39 = {
    'question': (
        'True or False: Mode collapse occurs when the discriminator fails to learn and the generator overfits the training data. (Probability of appearing on test: 0.871)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'False',
    'explanation': (
        'Mode collapse happens when the generator learns to produce a limited set of outputs that consistently fool the discriminator.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_GAN_40 = {
    'question': (
        'True or False: GANs can be used for semi-supervised learning by modifying the discriminator to output class labels. (Probability of appearing on test: 0.813)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'True',
    'explanation': (
        'By extending the discriminator to output class probabilities in addition to distinguishing real vs. fake samples, GANs can be adapted for semi-supervised learning.'
    ),
    'chapter_information': 'Gatech Lectures - GANs and VAEs'
}

question_VAE_41 = {
    'question': (
        'Which of the following statements best describes the role of the reparameterization trick in Variational Autoencoders (VAEs)? (Probability of appearing on test: 0.859)'
    ),
    'options_list': [
        'A. It approximates the intractable KL divergence term in the VAE objective.',
        'B. It ensures the encoder produces deterministic latent variables for accurate sampling.',
        'C. It moves the sampling operation to a differentiable layer, enabling backpropagation.',
        'D. It replaces the Gaussian prior $P(z)$ with a learned prior to improve regularization.',
        'E. It eliminates the need for stochastic gradient descent in training VAEs.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'The reparameterization trick allows gradients to flow through the sampling process by reformulating $z$ as a deterministic function '
        'of the mean, variance, and random noise.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}

question_VAE_42 = {
    'question': (
        'What is the primary purpose of the KL divergence term $D_{KL}(Q(z\\mid X) \\parallel P(z))$ in the VAE loss function? (Probability of appearing on test: 0.981)'
    ),
    'options_list': [
        'A. Ensure that latent variable $z$ perfectly reconstructs the input data $X$.',
        'B. Minimize the distance between the posterior $Q(z\\mid X)$ and the prior $P(z)$.',
        'C. Penalize high-dimensional latent representations to encourage sparsity.',
        'D. Maximize the likelihood of $P(X\\mid z)$ under the decoder network.',
        'E. Regularize the encoder to produce independent samples from a uniform distribution.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The KL divergence term enforces that the learned posterior $Q(z\\mid X)$ aligns with the prior $P(z)$, encouraging a smooth latent space '
        'for better generalization and sampling.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}

question_VAE_43 = {
    'question': (
        'When training a Variational Autoencoder, which of the following challenges is directly addressed by the use of a probabilistic decoder $P(X\\mid z)$? (Probability of appearing on test: 0.912)'
    ),
    'options_list': [
        'A. Overfitting due to the high-dimensional latent space.',
        'B. Ensuring that generated samples resemble the training data distribution.',
        'C. Handling the intractability of computing $P(X)$ exactly.',
        'D. Allowing discrete latent variables in the encoder.',
        'E. Guaranteeing that latent representations are disentangled.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The decoder $P(X\\mid z)$ generates $X$ given a sampled $z$, ensuring that reconstructed or generated data is consistent with the observed data distribution.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}

question_VAE_44 = {
    'question': (
        'Which of the following conditions would most likely result in poor performance of a VAE? (Probability of appearing on test: 0.913)'
    ),
    'options_list': [
        'A. Setting the KL divergence weight to zero during training.',
        'B. Using a Gaussian decoder for continuous data.',
        'C. Choosing a latent dimension smaller than the intrinsic data dimensionality.',
        'D. Using a diagonal covariance matrix for $Q(z\\mid X)$.',
        'E. Training on a dataset with imbalanced classes.'
    ],
    'correct_answer': 'A',
    'explanation': (
        'Without the KL divergence term, the encoder is not constrained to produce a distribution aligned with the prior, leading to poor sampling and generalization.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}

question_VAE_45 = {
    'question': (
        'Which of the following methods extends VAEs for tasks involving conditional generation (e.g., image completion)? (Probability of appearing on test: 0.763)'
    ),
    'options_list': [
        'A. Use of a multivariate Gaussian decoder.',
        'B. Training with augmented data to improve robustness.',
        'C. Incorporating a Conditional Variational Autoencoder (CVAE) that conditions both the encoder and decoder on input $X$.',
        'D. Replacing the standard KL divergence with Wasserstein loss.',
        'E. Including adversarial training objectives.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Conditional VAEs modify the generative process to explicitly condition on auxiliary inputs $X$, allowing for conditional generation of outputs.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}

question_VAE_46 = {
    'question': (
        'True or False: The encoder in a Variational Autoencoder is trained to output deterministic latent variables $z$. (Probability of appearing on test: 0.732)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'False',
    'explanation': (
        'The encoder outputs a distribution over $z$ (parameterized by $\\mu$ and $\\sigma$), not deterministic values.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}

question_VAE_47 = {
    'question': (
        'True or False: The reconstruction loss in a VAE objective function is always computed using the $\\ell_2$-norm between the input $X$ and its reconstruction. (Probability of appearing on test: 0.736)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'False',
    'explanation': (
        'The reconstruction loss depends on the output distribution; for binary data, a Bernoulli likelihood is often used with cross-entropy loss.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}

question_VAE_48 = {
    'question': (
        'True or False: VAEs assume that the posterior $Q(z\\mid X)$ is independent of the prior $P(z)$. (Probability of appearing on test: 0.791)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'False',
    'explanation': (
        'The KL divergence term explicitly encourages $Q(z\\mid X)$ to align with $P(z)$, thus enforcing dependence between them.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}

question_VAE_49 = {
    'question': (
        'True or False: Sampling $z \\sim N(\\mu, \\sigma^2)$ directly during training makes the VAE training objective non-differentiable. (Probability of appearing on test: 0.729)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'True',
    'explanation': (
        'Direct sampling is non-differentiable, which is why the reparameterization trick is necessary.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}

question_VAE_50 = {
    'question': (
        'True or False: Increasing the dimensionality of the latent space always improves the generative quality of a VAE. (Probability of appearing on test: 0.908)'
    ),
    'options_list': ['True', 'False'],
    'correct_answer': 'False',
    'explanation': (
        'Excessive latent dimensions can lead to overfitting, making the KL divergence term ineffective and resulting in poor generalization.'
    ),
    'chapter_information': 'Gatech Lectures - Variational Autoencoders'
}




KC_MPC_QUESTIONS = []
global_items = list(globals().items())
# print(global_items)

for name, value in global_items:
    if not name.startswith('_'):
        KC_MPC_QUESTIONS.append(value)

WEEK_19_MPC = KC_MPC_QUESTIONS[:-1]
