

monzersaleh_data_wrangling_question_1 = {
    'question': "What is the purpose of cross-validation in model evaluation?",
    'options_list': [
        'To increase the size of the training dataset.',
        'To estimate the prediction error and avoid overfitting.',
        'To remove outliers from the dataset.',
        'To perform hyperparameter tuning.'
    ],
    'correct_answer': 'To estimate the prediction error and avoid overfitting.',
    'explanation': "Cross-validation is used to estimate the prediction error of a model and help prevent overfitting by using different subsets of the data for training and testing.",
    'chapter_information': 'Monzersaleh Notes on Data Wrangling'
}

monzersaleh_data_wrangling_question_2 = {
    'question': "Which method can be used to handle class imbalance in a dataset?",
    'options_list': [
        'Dropping the minority class.',
        'Using the SMOTE technique.',
        'Ignoring the imbalance and proceeding with training.',
        'Removing all duplicate samples from the majority class.'
    ],
    'correct_answer': 'Using the SMOTE technique.',
    'explanation': "SMOTE (Synthetic Minority OverSampling Technique) is a method used to handle class imbalance by generating synthetic samples for the minority class.",
    'chapter_information': 'Monzersaleh Notes on Data Wrangling'
}

monzersaleh_data_wrangling_question_3 = {
    'question': "What is the main goal of data pre-processing in deep learning?",
    'options_list': [
        'To increase the dimensionality of the dataset.',
        'To improve model convergence by scaling and normalizing data.',
        'To create new features from the existing dataset.',
        'To decrease the modelâ€™s computational complexity.'
    ],
    'correct_answer': 'To improve model convergence by scaling and normalizing data.',
    'explanation': "Data pre-processing, such as scaling and normalizing, helps improve model convergence and stability during training.",
    'chapter_information': 'Monzersaleh Notes on Data Wrangling'
}

monzersaleh_data_wrangling_question_4 = {
    'question': "What does the term 'missing at random' imply in the context of missing data?",
    'options_list': [
        'The likelihood of a data observation being missing depends on other observed data features.',
        'The likelihood of a data observation being missing is completely random.',
        'The missing data is related to unobserved outcomes.',
        'The missing data can be ignored without any consequences.'
    ],
    'correct_answer': 'The likelihood of a data observation being missing depends on other observed data features.',
    'explanation': "'Missing at random' means that the likelihood of a data observation being missing depends on other observed features, which can guide the imputation process.",
    'chapter_information': 'Monzersaleh Notes on Data Wrangling'
}

monzersaleh_data_wrangling_question_5 = {
    'question': "What is the key advantage of using Focal Loss over Cross Entropy in classification tasks?",
    'options_list': [
        'It simplifies the model architecture.',
        'It down-weights easy examples, focusing more on hard examples.',
        'It reduces the size of the training dataset.',
        'It improves model interpretability.'
    ],
    'correct_answer': 'It down-weights easy examples, focusing more on hard examples.',
    'explanation': "Focal Loss is designed to down-weight easy examples, which allows the model to focus more on difficult and rare examples during training.",
    'chapter_information': 'Monzersaleh Notes on Data Wrangling'
}
