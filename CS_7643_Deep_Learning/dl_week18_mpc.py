
# question_gatech_MDP_1_10 = {
#     'question': (
#         'True or False: Self-supervised learning requires large quantities of labeled data to train models.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'Self-supervised learning creates pseudo-labels from unlabeled data and does not require human-labeled datasets, making it effective for large-scale unsupervised tasks.'
#     ),
#     'chapter_information': 'gatech MDP lecture'
# }

# question_Gatech_Lecture_1 = {
#     'question': (
#         'Which of the following statements correctly distinguishes semi-supervised, few-shot, and self-supervised learning?'
#     ),
#     'options_list': [
#         'A. Semi-supervised learning uses a few labeled examples and no unlabeled data, while few-shot learning requires both labeled and unlabeled data.',
#         'B. Few-shot learning assumes abundant unlabeled data, while self-supervised learning uses pseudo-labels for weak augmentations.',
#         'C. Semi-supervised learning assumes some labeled data and a larger set of unlabeled data, while few-shot learning focuses on minimal labeled examples per category and may rely on auxiliary datasets.',
#         'D. Self-supervised learning primarily uses clustering to predict pseudo-labels for augmentations of unlabeled data.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Semi-supervised learning combines labeled and unlabeled data, while few-shot learning focuses on training models '
#         'with very limited labeled examples, often leveraging auxiliary datasets. Self-supervised learning relies on surrogate tasks, not clustering.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Gatech_Lecture_2 = {
#     'question': (
#         'Which of the following best characterizes self-supervised learning?'
#     ),
#     'options_list': [
#         'A. Self-supervised learning uses clustering methods to group similar data points, assigning pseudo-labels based on distance metrics.',
#         'B. Self-supervised learning uses a task with synthetic labels derived from the data itself to learn representations.',
#         'C. Self-supervised learning requires a small amount of labeled data to bootstrap representation learning.',
#         'D. Self-supervised learning assumes the availability of a large auxiliary dataset with extensive labeling.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'Self-supervised learning creates surrogate tasks, such as predicting the rotation of an image, where the "labels" '
#         'are generated by transformations applied to the data.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Gatech_Lecture_3 = {
#     'question': (
#         'What distinguishes surrogate tasks in self-supervised learning from traditional supervised tasks?'
#     ),
#     'options_list': [
#         'A. Surrogate tasks are easier because they use synthetic labels.',
#         'B. Surrogate tasks are unrelated to the ultimate task but force networks to learn useful features.',
#         'C. Surrogate tasks are specific to semi-supervised learning and not used in self-supervised learning.',
#         'D. Surrogate tasks aim to cluster unlabeled data into meaningful groups.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'Surrogate tasks, such as predicting image rotations, are designed to train networks to learn representations that can transfer to other tasks.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Gatech_Lecture_4 = {
#     'question': (
#         'Which of the following is an example of self-supervised learning?'
#     ),
#     'options_list': [
#         'A. Training a network to predict the labels of weakly augmented images using cross-entropy loss.',
#         'B. Training a network to identify the direction of image rotation in unlabeled images.',
#         'C. Training a network using a mix of labeled and unlabeled data with pseudo-labels generated for unlabeled examples.',
#         'D. Training a network on 1-5 labeled examples per category and transferring to new categories.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'Predicting image rotations is a classic surrogate task used in self-supervised learning.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Gatech_Lecture_5 = {
#     'question': (
#         'In semi-supervised learning, what is the role of pseudo-labels?'
#     ),
#     'options_list': [
#         'A. They allow supervised learning methods to leverage weakly augmented labeled data.',
#         'B. They provide ground truth labels derived from human annotation.',
#         'C. They extend the training set by assigning confident predictions to unlabeled examples.',
#         'D. They are used exclusively to train clustering-based methods.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Pseudo-labels are high-confidence predictions on unlabeled data, which are treated as additional labeled data during training.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Gatech_Lecture_6 = {
#     'question': (
#         'True/False: Few-shot learning requires the presence of a large set of unlabeled data to perform transfer learning effectively.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'Few-shot learning typically relies on auxiliary labeled datasets rather than unlabeled data, though some variants can combine both.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Gatech_Lecture_7 = {
#     'question': (
#         'True/False: In self-supervised learning, surrogate tasks always predict labels for the ultimate task the network will solve.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'Surrogate tasks generate synthetic labels for intermediate objectives, not the ultimate task.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Gatech_Lecture_8 = {
#     'question': (
#         'True/False: Semi-supervised learning involves training with a combination of labeled and unlabeled data, often using pseudo-labeling to expand the labeled dataset.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'Semi-supervised learning combines labeled and unlabeled data, using pseudo-labeling as a common strategy.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Gatech_Lecture_9 = {
#     'question': (
#         'True/False: Self-supervised learning can reduce reliance on labeled data by learning effective feature representations from unlabeled data.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'Self-supervised learning leverages unlabeled data and surrogate tasks to learn representations transferable to downstream tasks.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Gatech_Lecture_10 = {
#     'question': (
#         'True/False: Weak and strong augmentations are commonly used together in self-supervised learning to enhance feature robustness.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'Combining weak and strong augmentations helps train models to produce consistent representations under varied transformations.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }


# question_Semi_Supervised_1 = {
#     'question': (
#         'In semi-supervised learning, which of the following combinations of labeled and unlabeled data is typically assumed?'
#     ),
#     'options_list': [
#         'A. A small amount of labeled data and no unlabeled data.',
#         'B. A large amount of labeled data and a small amount of unlabeled data.',
#         'C. A small amount of labeled data and a much larger amount of unlabeled data.',
#         'D. Equal amounts of labeled and unlabeled data.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Semi-supervised learning leverages a small labeled dataset combined with a much larger unlabeled dataset to improve learning performance.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Semi_Supervised_2 = {
#     'question': (
#         'What is the primary purpose of weak augmentation in the FixMatch algorithm?'
#     ),
#     'options_list': [
#         'A. To increase the diversity of the dataset.',
#         'B. To generate challenging inputs for the model.',
#         'C. To create confident pseudo-labels for training the strongly augmented branch.',
#         'D. To prevent overfitting to the labeled dataset.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Weak augmentation ensures that pseudo-labels are reliable by avoiding significant distortions, allowing them to be used as ground truth for training the strongly augmented data.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Semi_Supervised_3 = {
#     'question': (
#         'Which of the following describes a key difference between semi-supervised learning and self-supervised learning?'
#     ),
#     'options_list': [
#         'A. Semi-supervised learning uses only labeled data, while self-supervised learning uses unlabeled data.',
#         'B. Semi-supervised learning uses a mix of labeled and unlabeled data, while self-supervised learning creates labels from the data itself.',
#         'C. Semi-supervised learning relies on surrogate tasks, while self-supervised learning uses pseudo-labels.',
#         'D. Semi-supervised learning is limited to small datasets, while self-supervised learning scales to large datasets.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'Semi-supervised learning combines labeled and unlabeled data, while self-supervised learning generates pseudo-labels or surrogate tasks directly from the data.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Self_Supervised_1 = {
#     'question': (
#         'Which of the following is NOT a type of self-supervised surrogate task?'
#     ),
#     'options_list': [
#         'A. Rotation prediction.',
#         'B. Autoencoder reconstruction.',
#         'C. Data clustering.',
#         'D. Dimensionality reduction.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Clustering is a classical unsupervised learning technique, not a self-supervised surrogate task.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Semi_Supervised_4 = {
#     'question': (
#         'What is the main challenge addressed by the FixMatch algorithm in semi-supervised learning?'
#     ),
#     'options_list': [
#         'A. Reducing computation time for large datasets.',
#         'B. Handling noisy labels in the labeled dataset.',
#         'C. Generating reliable pseudo-labels from unlabeled data.',
#         'D. Learning robust features without data augmentation.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'FixMatch focuses on generating reliable pseudo-labels through weak augmentation and using them to train the model with strongly augmented data.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Semi_Supervised_5 = {
#     'question': (
#         'True/False: Pseudo-labeling in semi-supervised learning can fail if the confidence threshold is too strict or too lenient.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'A strict threshold results in fewer pseudo-labels, limiting learning, while a lenient threshold introduces noisy labels, degrading model performance.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Self_Supervised_2 = {
#     'question': (
#         'True/False: Self-supervised learning tasks always require labeled data for training.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'Self-supervised learning creates tasks and labels from unlabeled data itself, making it independent of labeled datasets.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Semi_Supervised_6 = {
#     'question': (
#         'True/False: Label propagation in semi-supervised learning assumes that nearby examples in the feature space should share similar labels.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'Label propagation leverages the clustering assumption to propagate labels from labeled examples to nearby unlabeled ones.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Semi_Supervised_7 = {
#     'question': (
#         'True/False: Strong augmentation in FixMatch is used to challenge the model and improve its robustness.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'Strong augmentation generates challenging inputs, forcing the model to generalize effectively.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

# question_Semi_Supervised_8 = {
#     'question': (
#         'True/False: Semi-supervised learning methods like FixMatch are typically less effective on real-world datasets compared to academic datasets.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'Real-world datasets often have noise and complexity that reduce the performance gains achieved by semi-supervised methods like FixMatch.'
#     ),
#     'chapter_information': 'Gatech Lectures'
# }

question_gatech_MDP_1_10 = {
    'question': (
        'True or False: Self-supervised learning requires large quantities of labeled data to train models. '
        '(Probability of appearing on test: 0.865)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'Self-supervised learning creates pseudo-labels from unlabeled data and does not require human-labeled datasets, making it effective for large-scale unsupervised tasks.'
    ),
    'chapter_information': 'gatech MDP lecture'
}

question_Gatech_Lecture_1 = {
    'question': (
        'Which of the following statements correctly distinguishes semi-supervised, few-shot, and self-supervised learning? '
        '(Probability of appearing on test: 0.115)'
    ),
    'options_list': [
        'A. Semi-supervised learning uses a few labeled examples and no unlabeled data, while few-shot learning requires both labeled and unlabeled data.',
        'B. Few-shot learning assumes abundant unlabeled data, while self-supervised learning uses pseudo-labels for weak augmentations.',
        'C. Semi-supervised learning assumes some labeled data and a larger set of unlabeled data, while few-shot learning focuses on minimal labeled examples per category and may rely on auxiliary datasets.',
        'D. Self-supervised learning primarily uses clustering to predict pseudo-labels for augmentations of unlabeled data.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Semi-supervised learning combines labeled and unlabeled data, while few-shot learning focuses on training models '
        'with very limited labeled examples, often leveraging auxiliary datasets. Self-supervised learning relies on surrogate tasks, not clustering.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Gatech_Lecture_2 = {
    'question': (
        'Which of the following best characterizes self-supervised learning? '
        '(Probability of appearing on test: 0.766)'
    ),
    'options_list': [
        'A. Self-supervised learning uses clustering methods to group similar data points, assigning pseudo-labels based on distance metrics.',
        'B. Self-supervised learning uses a task with synthetic labels derived from the data itself to learn representations.',
        'C. Self-supervised learning requires a small amount of labeled data to bootstrap representation learning.',
        'D. Self-supervised learning assumes the availability of a large auxiliary dataset with extensive labeling.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'Self-supervised learning creates surrogate tasks, such as predicting the rotation of an image, where the "labels" '
        'are generated by transformations applied to the data.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Gatech_Lecture_3 = {
    'question': (
        'What distinguishes surrogate tasks in self-supervised learning from traditional supervised tasks? '
        '(Probability of appearing on test: 0.482)'
    ),
    'options_list': [
        'A. Surrogate tasks are easier because they use synthetic labels.',
        'B. Surrogate tasks are unrelated to the ultimate task but force networks to learn useful features.',
        'C. Surrogate tasks are specific to semi-supervised learning and not used in self-supervised learning.',
        'D. Surrogate tasks aim to cluster unlabeled data into meaningful groups.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'Surrogate tasks, such as predicting image rotations, are designed to train networks to learn representations that can transfer to other tasks.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Gatech_Lecture_4 = {
    'question': (
        'Which of the following is an example of self-supervised learning? '
        '(Probability of appearing on test: 0.772)'
    ),
    'options_list': [
        'A. Training a network to predict the labels of weakly augmented images using cross-entropy loss.',
        'B. Training a network to identify the direction of image rotation in unlabeled images.',
        'C. Training a network using a mix of labeled and unlabeled data with pseudo-labels generated for unlabeled examples.',
        'D. Training a network on 1-5 labeled examples per category and transferring to new categories.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'Predicting image rotations is a classic surrogate task used in self-supervised learning.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Gatech_Lecture_5 = {
    'question': (
        'In semi-supervised learning, what is the role of pseudo-labels? '
        '(Probability of appearing on test: 0.827)'
    ),
    'options_list': [
        'A. They allow supervised learning methods to leverage weakly augmented labeled data.',
        'B. They provide ground truth labels derived from human annotation.',
        'C. They extend the training set by assigning confident predictions to unlabeled examples.',
        'D. They are used exclusively to train clustering-based methods.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Pseudo-labels are high-confidence predictions on unlabeled data, which are treated as additional labeled data during training.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Gatech_Lecture_6 = {
    'question': (
        'True/False: Few-shot learning requires the presence of a large set of unlabeled data to perform transfer learning effectively. '
        '(Probability of appearing on test: 0.791)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'Few-shot learning typically relies on auxiliary labeled datasets rather than unlabeled data, though some variants can combine both.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Gatech_Lecture_7 = {
    'question': (
        'True/False: In self-supervised learning, surrogate tasks always predict labels for the ultimate task the network will solve. '
        '(Probability of appearing on test: 0.758)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'Surrogate tasks generate synthetic labels for intermediate objectives, not the ultimate task.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Gatech_Lecture_8 = {
    'question': (
        'True/False: Semi-supervised learning involves training with a combination of labeled and unlabeled data, often using pseudo-labeling to expand the labeled dataset. '
        '(Probability of appearing on test: 0.674)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'Semi-supervised learning combines labeled and unlabeled data, using pseudo-labeling as a common strategy.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Gatech_Lecture_9 = {
    'question': (
        'True/False: Self-supervised learning can reduce reliance on labeled data by learning effective feature representations from unlabeled data. '
        '(Probability of appearing on test: 0.824)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'Self-supervised learning leverages unlabeled data and surrogate tasks to learn representations transferable to downstream tasks.'
    ),
    'chapter_information': 'Gatech Lectures'
}


question_Gatech_Lecture_10 = {
    'question': (
        'True/False: Weak and strong augmentations are commonly used together in self-supervised learning to enhance feature robustness. '
        '(Probability of appearing on test: 0.948)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'Combining weak and strong augmentations helps train models to produce consistent representations under varied transformations.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Semi_Supervised_1 = {
    'question': (
        'In semi-supervised learning, which of the following combinations of labeled and unlabeled data is typically assumed? '
        '(Probability of appearing on test: 0.345)'
    ),
    'options_list': [
        'A. A small amount of labeled data and no unlabeled data.',
        'B. A large amount of labeled data and a small amount of unlabeled data.',
        'C. A small amount of labeled data and a much larger amount of unlabeled data.',
        'D. Equal amounts of labeled and unlabeled data.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Semi-supervised learning leverages a small labeled dataset combined with a much larger unlabeled dataset to improve learning performance.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Semi_Supervised_2 = {
    'question': (
        'What is the primary purpose of weak augmentation in the FixMatch algorithm? '
        '(Probability of appearing on test: 0.801)'
    ),
    'options_list': [
        'A. To increase the diversity of the dataset.',
        'B. To generate challenging inputs for the model.',
        'C. To create confident pseudo-labels for training the strongly augmented branch.',
        'D. To prevent overfitting to the labeled dataset.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Weak augmentation ensures that pseudo-labels are reliable by avoiding significant distortions, allowing them to be used as ground truth for training the strongly augmented data.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Semi_Supervised_3 = {
    'question': (
        'Which of the following describes a key difference between semi-supervised learning and self-supervised learning? '
        '(Probability of appearing on test: 0.089)'
    ),
    'options_list': [
        'A. Semi-supervised learning uses only labeled data, while self-supervised learning uses unlabeled data.',
        'B. Semi-supervised learning uses a mix of labeled and unlabeled data, while self-supervised learning creates labels from the data itself.',
        'C. Semi-supervised learning relies on surrogate tasks, while self-supervised learning uses pseudo-labels.',
        'D. Semi-supervised learning is limited to small datasets, while self-supervised learning scales to large datasets.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'Semi-supervised learning combines labeled and unlabeled data, while self-supervised learning generates pseudo-labels or surrogate tasks directly from the data.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Self_Supervised_1 = {
    'question': (
        'Which of the following is NOT a type of self-supervised surrogate task? '
        '(Probability of appearing on test: 0.829)'
    ),
    'options_list': [
        'A. Rotation prediction.',
        'B. Autoencoder reconstruction.',
        'C. Data clustering.',
        'D. Dimensionality reduction.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Clustering is a classical unsupervised learning technique, not a self-supervised surrogate task.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Semi_Supervised_4 = {
    'question': (
        'What is the main challenge addressed by the FixMatch algorithm in semi-supervised learning? '
        '(Probability of appearing on test: 0.744)'
    ),
    'options_list': [
        'A. Reducing computation time for large datasets.',
        'B. Handling noisy labels in the labeled dataset.',
        'C. Generating reliable pseudo-labels from unlabeled data.',
        'D. Learning robust features without data augmentation.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'FixMatch focuses on generating reliable pseudo-labels through weak augmentation and using them to train the model with strongly augmented data.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Semi_Supervised_5 = {
    'question': (
        'True/False: Pseudo-labeling in semi-supervised learning can fail if the confidence threshold is too strict or too lenient. '
        '(Probability of appearing on test: 0.841)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'A strict threshold results in fewer pseudo-labels, limiting learning, while a lenient threshold introduces noisy labels, degrading model performance.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Self_Supervised_2 = {
    'question': (
        'True/False: Self-supervised learning tasks always require labeled data for training. '
        '(Probability of appearing on test: 0.931)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'Self-supervised learning creates tasks and labels from unlabeled data itself, making it independent of labeled datasets.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Semi_Supervised_6 = {
    'question': (
        'True/False: Label propagation in semi-supervised learning assumes that nearby examples in the feature space should share similar labels. '
        '(Probability of appearing on test: 0.854)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'Label propagation leverages the clustering assumption to propagate labels from labeled examples to nearby unlabeled ones.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Semi_Supervised_7 = {
    'question': (
        'True/False: Strong augmentation in FixMatch is used to challenge the model and improve its robustness. '
        '(Probability of appearing on test: 0.868)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'Strong augmentation generates challenging inputs, forcing the model to generalize effectively.'
    ),
    'chapter_information': 'Gatech Lectures'
}

question_Semi_Supervised_8 = {
    'question': (
        'True/False: Semi-supervised learning methods like FixMatch are typically less effective on real-world datasets compared to academic datasets. '
        '(Probability of appearing on test: 0.858)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'Real-world datasets often have noise and complexity that reduce the performance gains achieved by semi-supervised methods like FixMatch.'
    ),
    'chapter_information': 'Gatech Lectures'
}

# question_Gatech_Lecture_FewShot_1 = {
#     'question': (
#         'Which of the following statements best describes a key limitation of traditional fine-tuning approaches in few-shot learning scenarios?'
#     ),
#     'options_list': [
#         'A. Fine-tuning on a small support set always leads to overfitting because the feature extractor is frozen.',
#         'B. Traditional fine-tuning does not account for the difference between the number of classes during training and testing.',
#         'C. Fine-tuning fails because it optimizes for the end-way tasks encountered during inference time.',
#         'D. The lack of alignment between training and testing tasks in fine-tuning impedes performance in few-shot learning.',
#         'E. Fine-tuning methods inherently cannot handle more than five classes in few-shot learning tasks.'
#     ],
#     'correct_answer': 'D',
#     'explanation': (
#         'Traditional fine-tuning does not account for the specific tasks encountered during inference time in few-shot learning, '
#         'such as the n-way tasks with limited examples per class. This lack of alignment between training and testing tasks can '
#         'impede performance because the model is not optimized for the few-shot scenario.'
#     ),
#     'chapter_information': 'Gatech Lectures - Few-Shot Learning'
# }

# question_Gatech_Lecture_FewShot_2 = {
#     'question': (
#         'In the context of few-shot learning, which of the following is NOT an advantage of using cosine similarity-based classifiers over standard linear classifiers?'
#     ),
#     'options_list': [
#         'A. They focus on angular differences between feature vectors, which may improve discrimination among a small number of classes.',
#         'B. They inherently normalize feature vectors, reducing the impact of vector magnitude disparities.',
#         'C. They adapt better to scenarios with thousands of categories due to their reliance on cosine similarity.',
#         'D. They can provide better performance when discriminating across a small number of classes.',
#         'E. They interpret weight vectors as prototypes, enhancing class representation in low-data regimes.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Cosine similarity-based classifiers do not necessarily adapt better to scenarios with thousands of categories. They are '
#         'advantageous when discriminating across a small number of classes due to their focus on angular differences.'
#     ),
#     'chapter_information': 'Gatech Lectures - Few-Shot Learning'
# }

# question_Gatech_Lecture_FewShot_3 = {
#     'question': (
#         'True or False: In meta-learning approaches like MAML (Model-Agnostic Meta-Learning), the primary goal is to learn a flexible update rule that replaces gradient descent with a learned optimizer, thereby improving convergence on few-shot tasks.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'In MAML, the primary goal is to learn a good parameter initialization that can be quickly adapted to new tasks using '
#         'standard gradient descent, not to replace gradient descent with a learned optimizer.'
#     ),
#     'chapter_information': 'Gatech Lectures - Few-Shot Learning'
# }

# question_Gatech_Lecture_FewShot_4 = {
#     'question': (
#         'Which of the following best explains why meta-training involves simulating end-way tasks during the training phase in few-shot learning?'
#     ),
#     'options_list': [
#         'A. To increase the number of classes the model can handle during inference.',
#         'B. To ensure the model only learns from classes that will appear during testing.',
#         'C. To align the training process with the testing scenario, improving generalization to new tasks.',
#         'D. To reduce computational complexity by limiting the number of classes considered.',
#         'E. To enable the model to ignore classes from the base dataset during fine-tuning.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Simulating n-way tasks during meta-training aligns the training process with the testing scenario, which involves making '
#         'predictions on new tasks with n classes and few examples per class. This alignment improves the model\'s ability to generalize '
#         'to new tasks.'
#     ),
#     'chapter_information': 'Gatech Lectures - Few-Shot Learning'
# }

# question_Gatech_Lecture_FewShot_5 = {
#     'question': (
#         'In the context of few-shot learning, the prototypical network approach differs from the matching network approach primarily because it:'
#     ),
#     'options_list': [
#         'A. Uses a learned metric instead of a hand-coded similarity function.',
#         'B. Compares query items to mean embeddings of each class rather than individual support examples.',
#         'C. Employs an LSTM-based meta-learner to update the feature extractor.',
#         'D. Requires a larger support set per class to function effectively.',
#         'E. Utilizes a black-box optimization method instead of gradient-based updates.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'Prototypical networks compute mean embeddings (prototypes) for each class and compare query items to these prototypes, '
#         'unlike matching networks that compare query items to individual support examples.'
#     ),
#     'chapter_information': 'Gatech Lectures - Few-Shot Learning'
# }

# question_Gatech_Lecture_FewShot_6 = {
#     'question': (
#         'True or False: One of the key benefits of model-agnostic meta-learning (MAML) is that it modifies the standard gradient descent update rule to include adaptive learning rates and weight decay, leading to better performance on few-shot learning tasks.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'MAML does not modify the standard gradient descent update rule. Instead, it focuses on learning a good initialization '
#         'for the model parameters, such that applying standard gradient descent with a small amount of data leads to good performance on new tasks.'
#     ),
#     'chapter_information': 'Gatech Lectures - Few-Shot Learning'
# }

# question_Gatech_Lecture_FewShot_7 = {
#     'question': (
#         'Which of the following is a primary reason for backpropagating through the gradient descent steps in meta-learning algorithms like MAML?'
#     ),
#     'options_list': [
#         'A. To learn a new optimizer that replaces gradient descent entirely.',
#         'B. To adjust the learning rate schedule for faster convergence during training.',
#         'C. To update the initialization parameters so that minimal fine-tuning is needed on new tasks.',
#         'D. To incorporate unlabeled data into the training process through self-supervised learning.',
#         'E. To enable the model to handle larger support sets without overfitting.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'By backpropagating through the gradient descent steps, meta-learning algorithms like MAML adjust the initialization parameters '
#         'so that minimal fine-tuning (few gradient steps) is needed to achieve good performance on new tasks.'
#     ),
#     'chapter_information': 'Gatech Lectures - Few-Shot Learning'
# }

# question_Gatech_Lecture_FewShot_8 = {
#     'question': (
#         'In few-shot learning, which statement best captures the relationship between the support set and the query set during meta-training?'
#     ),
#     'options_list': [
#         'A. The support set contains unlabeled data used to fine-tune the model, while the query set contains labeled data for evaluation.',
#         'B. Both the support set and query set are subsets of the base dataset and contain overlapping classes.',
#         'C. The support set is used to learn feature embeddings, and the query set is used to update the feature extractor.',
#         'D. The support set provides labeled examples for new classes, and the query set is used to assess the model\'s ability to generalize to these classes.',
#         'E. The support set and query set are merged to increase the number of examples per class for training.'
#     ],
#     'correct_answer': 'D',
#     'explanation': (
#         'In meta-training, the support set provides labeled examples for new classes, and the query set is used to assess the model\'s '
#         'ability to generalize to these classes, simulating the few-shot learning scenario.'
#     ),
#     'chapter_information': 'Gatech Lectures - Few-Shot Learning'
# }

# question_Gatech_Lecture_FewShot_9 = {
#     'question': (
#         'True or False: Meta-learning approaches in few-shot learning exclusively focus on improving the feature extractor to prevent overfitting on small support sets, without modifying the classifier component.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'False',
#     'explanation': (
#         'Meta-learning approaches often modify both the feature extractor and the classifier component. For example, methods like MAML '
#         'learn an initialization for the entire model (feature extractor and classifier), and some approaches focus on learning adaptive classifiers suitable for few-shot tasks.'
#     ),
#     'chapter_information': 'Gatech Lectures - Few-Shot Learning'
# }


# question_Gatech_Lecture_SelfSupervised_1 = {
#     'question': (
#         'Which of the following statements best describes the key difference between unsupervised learning and self-supervised learning?'
#     ),
#     'options_list': [
#         'A. Unsupervised learning uses labeled data, while self-supervised learning uses unlabeled data.',
#         'B. In unsupervised learning, the model learns to predict labels, whereas in self-supervised learning, the model learns to reconstruct its input.',
#         'C. Self-supervised learning involves creating surrogate tasks where the labels are derived from the data itself, while unsupervised learning does not rely on any labels.',
#         'D. Unsupervised learning is a subset of self-supervised learning.',
#         'E. Self-supervised learning requires small amounts of labeled data, while unsupervised learning requires large amounts of unlabeled data.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Self-supervised learning involves creating surrogate tasks where labels are derived from the data itself (e.g., predicting rotation, colorization). '
#         'Unsupervised learning, on the other hand, does not use any labels, including those generated from the data.'
#     ),
#     'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
# }

# question_Gatech_Lecture_SemiSupervised_1 = {
#     'question': (
#         'In semi-supervised learning, what type of data is typically used to improve model performance?'
#     ),
#     'options_list': [
#         'A. Only large amounts of labeled data.',
#         'B. Only small amounts of unlabeled data.',
#         'C. A combination of a small amount of labeled data and a large amount of unlabeled data.',
#         'D. Only data with pseudo-labels generated by clustering algorithms.',
#         'E. Data augmented by synthetic labels generated through data augmentation.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Semi-supervised learning leverages a small amount of labeled data along with a large amount of unlabeled data to improve model performance.'
#     ),
#     'chapter_information': 'Gatech Lectures - Semi-Supervised Learning'
# }

# question_Gatech_Lecture_SelfSupervised_2 = {
#     'question': (
#         'Which of the following is an example of a self-supervised learning task?'
#     ),
#     'options_list': [
#         'A. Training a classifier using labeled images to predict object categories.',
#         'B. Using K-Means clustering to group unlabeled data into clusters.',
#         'C. Predicting the rotation angle applied to an image as an output.',
#         'D. Using principal component analysis to reduce the dimensionality of the data.',
#         'E. Fine-tuning a pre-trained model on a new task with few labeled examples.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Predicting the rotation angle applied to an image is a self-supervised task where labels are derived from the data transformations.'
#     ),
#     'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
# }

# question_Gatech_Lecture_Unsupervised_1 = {
#     'question': (
#         'In the context of autoencoders used in unsupervised learning, which of the following statements is true?'
#     ),
#     'options_list': [
#         'A. Autoencoders require labeled data to learn effective feature representations.',
#         'B. The loss function used in autoencoders is typically cross-entropy loss.',
#         'C. Autoencoders learn to reconstruct the input data by encoding it into a lower-dimensional space and then decoding it back.',
#         'D. Autoencoders are used to cluster data into meaningful groups.',
#         'E. Autoencoders cannot be used to pre-train models for downstream tasks.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Autoencoders compress input data into a lower-dimensional representation (encoding) and then reconstruct the original data (decoding).'
#     ),
#     'chapter_information': 'Gatech Lectures - Unsupervised Learning'
# }

# question_Gatech_Lecture_Unsupervised_2 = {
#     'question': (
#         'Which of the following best describes the "clustering assumption" in unsupervised learning?'
#     ),
#     'options_list': [
#         'A. Data points are uniformly distributed across the feature space.',
#         'B. High-density regions in feature space correspond to clusters of similar data points.',
#         'C. Clusters are formed by randomly grouping data points together.',
#         'D. Clustering algorithms always produce balanced clusters with equal numbers of data points.',
#         'E. Clustering is only effective when data is labeled.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'The clustering assumption states that data points in high-density regions are similar and form clusters.'
#     ),
#     'chapter_information': 'Gatech Lectures - Unsupervised Learning'
# }

# question_Gatech_Lecture_SelfSupervised_3 = {
#     'question': (
#         'In the self-supervised task of colorization, what is the primary goal for the neural network?'
#     ),
#     'options_list': [
#         'A. To classify images into different object categories.',
#         'B. To convert color images to grayscale images.',
#         'C. To reconstruct the color channels of an image given its grayscale version.',
#         'D. To segment the image into different regions based on color.',
#         'E. To predict the rotation angle applied to an image.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'The goal of colorization in self-supervised learning is to reconstruct the color channels from a grayscale image.'
#     ),
#     'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
# }

# question_Gatech_Lecture_SelfSupervised_4 = {
#     'question': (
#         'Which loss function is most appropriate for the self-supervised task of rotation prediction where the model predicts one of four discrete rotation angles applied to an image?'
#     ),
#     'options_list': [
#         'A. Mean squared error loss.',
#         'B. Cross-entropy classification loss.',
#         'C. Hinge loss.',
#         'D. Mean absolute error loss.',
#         'E. Triplet loss.'
#     ],
#     'correct_answer': 'B',
#     'explanation': (
#         'Since rotation prediction involves classifying discrete rotation angles, cross-entropy loss is appropriate.'
#     ),
#     'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
# }

# question_Gatech_Lecture_SelfSupervised_5 = {
#     'question': (
#         'In the context of instance discrimination as a self-supervised task, which of the following statements is correct?'
#     ),
#     'options_list': [
#         'A. The model learns to discriminate between different classes of objects using labeled data.',
#         'B. The model learns to bring together features of different images of the same object class.',
#         'C. The model learns to bring together features of different augmentations of the same image and push apart features of other images.',
#         'D. The model clusters images based on their semantic content without any augmentations.',
#         'E. The model uses supervised labels to distinguish between different instances.'
#     ],
#     'correct_answer': 'C',
#     'explanation': (
#         'Instance discrimination trains the model to pull together features from different augmentations of the same image and push apart features from other images.'
#     ),
#     'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
# }

# question_Gatech_Lecture_SelfSupervised_6 = {
#     'question': (
#         'True or False: Data augmentation plays a key role in self-supervised learning tasks, and the choice of augmentation methods can significantly impact the effectiveness of the learned features.'
#     ),
#     'options_list': [
#         'True',
#         'False'
#     ],
#     'correct_answer': 'True',
#     'explanation': (
#         'Data augmentation is crucial in self-supervised learning, as it affects the model\'s ability to learn meaningful invariances and representations.'
#     ),
#     'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
# }

# question_Gatech_Lecture_Unsupervised_3 = {
#     'question': (
#         'Which of the following is NOT a common challenge when using clustering-based methods in unsupervised feature learning?'
#     ),
#     'options_list': [
#         'A. The possibility of empty clusters where no data points are assigned.',
#         'B. The tendency of the neural network to assign all data points to a single cluster.',
#         'C. Imbalanced clusters where some clusters have many more data points than others.',
#         'D. The inability to use backpropagation to update model parameters.',
#         'E. The requirement to balance the feature learning and clustering processes.'
#     ],
#     'correct_answer': 'D',
#     'explanation': (
#         'Backpropagation is used to update model parameters in clustering-based methods; it\'s not a challenge.'
#     ),
#     'chapter_information': 'Gatech Lectures - Unsupervised Learning'
# }

question_Gatech_Lecture_Unsupervised_1 = {
    'question': (
        'In the context of autoencoders used in unsupervised learning, which of the following statements is true? (Probability of appearing on test: 0.791)'
    ),
    'options_list': [
        'A. Autoencoders require labeled data to learn effective feature representations.',
        'B. The loss function used in autoencoders is typically cross-entropy loss.',
        'C. Autoencoders learn to reconstruct the input data by encoding it into a lower-dimensional space and then decoding it back.',
        'D. Autoencoders are used to cluster data into meaningful groups.',
        'E. Autoencoders cannot be used to pre-train models for downstream tasks.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Autoencoders compress input data into a lower-dimensional representation (encoding) and then reconstruct the original data (decoding).'
    ),
    'chapter_information': 'Gatech Lectures - Unsupervised Learning'
}

question_Gatech_Lecture_Unsupervised_2 = {
    'question': (
        'Which of the following best describes the "clustering assumption" in unsupervised learning? (Probability of appearing on test: 0.854)'
    ),
    'options_list': [
        'A. Data points are uniformly distributed across the feature space.',
        'B. High-density regions in feature space correspond to clusters of similar data points.',
        'C. Clusters are formed by randomly grouping data points together.',
        'D. Clustering algorithms always produce balanced clusters with equal numbers of data points.',
        'E. Clustering is only effective when data is labeled.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'The clustering assumption states that data points in high-density regions are similar and form clusters.'
    ),
    'chapter_information': 'Gatech Lectures - Unsupervised Learning'
}

question_Gatech_Lecture_SelfSupervised_3 = {
    'question': (
        'In the self-supervised task of colorization, what is the primary goal for the neural network? (Probability of appearing on test: 0.815)'
    ),
    'options_list': [
        'A. To classify images into different object categories.',
        'B. To convert color images to grayscale images.',
        'C. To reconstruct the color channels of an image given its grayscale version.',
        'D. To segment the image into different regions based on color.',
        'E. To predict the rotation angle applied to an image.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'The goal of colorization in self-supervised learning is to reconstruct the color channels from a grayscale image.'
    ),
    'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
}

question_Gatech_Lecture_SelfSupervised_4 = {
    'question': (
        'Which loss function is most appropriate for the self-supervised task of rotation prediction where the model predicts one of four discrete rotation angles applied to an image? (Probability of appearing on test: 0.907)'
    ),
    'options_list': [
        'A. Mean squared error loss.',
        'B. Cross-entropy classification loss.',
        'C. Hinge loss.',
        'D. Mean absolute error loss.',
        'E. Triplet loss.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'Since rotation prediction involves classifying discrete rotation angles, cross-entropy loss is appropriate.'
    ),
    'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
}

question_Gatech_Lecture_SelfSupervised_5 = {
    'question': (
        'In the context of instance discrimination as a self-supervised task, which of the following statements is correct? (Probability of appearing on test: 0.612)'
    ),
    'options_list': [
        'A. The model learns to discriminate between different classes of objects using labeled data.',
        'B. The model learns to bring together features of different images of the same object class.',
        'C. The model learns to bring together features of different augmentations of the same image and push apart features of other images.',
        'D. The model clusters images based on their semantic content without any augmentations.',
        'E. The model uses supervised labels to distinguish between different instances.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Instance discrimination trains the model to pull together features from different augmentations of the same image and push apart features from other images.'
    ),
    'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
}

question_Gatech_Lecture_SelfSupervised_6 = {
    'question': (
        'True or False: Data augmentation plays a key role in self-supervised learning tasks, and the choice of augmentation methods can significantly impact the effectiveness of the learned features. (Probability of appearing on test: 0.709)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'True',
    'explanation': (
        'Data augmentation is crucial in self-supervised learning, as it affects the model\'s ability to learn meaningful invariances and representations.'
    ),
    'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
}

question_Gatech_Lecture_Unsupervised_3 = {
    'question': (
        'Which of the following is NOT a common challenge when using clustering-based methods in unsupervised feature learning? (Probability of appearing on test: 0.846)'
    ),
    'options_list': [
        'A. The possibility of empty clusters where no data points are assigned.',
        'B. The tendency of the neural network to assign all data points to a single cluster.',
        'C. Imbalanced clusters where some clusters have many more data points than others.',
        'D. The inability to use backpropagation to update model parameters.',
        'E. The requirement to balance the feature learning and clustering processes.'
    ],
    'correct_answer': 'D',
    'explanation': (
        'Backpropagation is used to update model parameters in clustering-based methods; it\'s not a challenge.'
    ),
    'chapter_information': 'Gatech Lectures - Unsupervised Learning'
}


question_Gatech_Lecture_FewShot_1 = {
    'question': (
        'Which of the following statements best describes a key limitation of traditional fine-tuning approaches in few-shot learning scenarios? (Probability of appearing on test: 0.576)'
    ),
    'options_list': [
        'A. Fine-tuning on a small support set always leads to overfitting because the feature extractor is frozen.',
        'B. Traditional fine-tuning does not account for the difference between the number of classes during training and testing.',
        'C. Fine-tuning fails because it optimizes for the end-way tasks encountered during inference time.',
        'D. The lack of alignment between training and testing tasks in fine-tuning impedes performance in few-shot learning.',
        'E. Fine-tuning methods inherently cannot handle more than five classes in few-shot learning tasks.'
    ],
    'correct_answer': 'D',
    'explanation': (
        'Traditional fine-tuning does not account for the specific tasks encountered during inference time in few-shot learning, '
        'such as the n-way tasks with limited examples per class. This lack of alignment between training and testing tasks can '
        'impede performance because the model is not optimized for the few-shot scenario.'
    ),
    'chapter_information': 'Gatech Lectures - Few-Shot Learning'
}

question_Gatech_Lecture_FewShot_2 = {
    'question': (
        'In the context of few-shot learning, which of the following is NOT an advantage of using cosine similarity-based classifiers over standard linear classifiers? (Probability of appearing on test: 0.830)'
    ),
    'options_list': [
        'A. They focus on angular differences between feature vectors, which may improve discrimination among a small number of classes.',
        'B. They inherently normalize feature vectors, reducing the impact of vector magnitude disparities.',
        'C. They adapt better to scenarios with thousands of categories due to their reliance on cosine similarity.',
        'D. They can provide better performance when discriminating across a small number of classes.',
        'E. They interpret weight vectors as prototypes, enhancing class representation in low-data regimes.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Cosine similarity-based classifiers do not necessarily adapt better to scenarios with thousands of categories. They are '
        'advantageous when discriminating across a small number of classes due to their focus on angular differences.'
    ),
    'chapter_information': 'Gatech Lectures - Few-Shot Learning'
}

question_Gatech_Lecture_FewShot_3 = {
    'question': (
        'True or False: In meta-learning approaches like MAML (Model-Agnostic Meta-Learning), the primary goal is to learn a flexible update rule that replaces gradient descent with a learned optimizer, thereby improving convergence on few-shot tasks. (Probability of appearing on test: 0.755)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'In MAML, the primary goal is to learn a good parameter initialization that can be quickly adapted to new tasks using '
        'standard gradient descent, not to replace gradient descent with a learned optimizer.'
    ),
    'chapter_information': 'Gatech Lectures - Few-Shot Learning'
}

question_Gatech_Lecture_FewShot_4 = {
    'question': (
        'Which of the following best explains why meta-training involves simulating end-way tasks during the training phase in few-shot learning? (Probability of appearing on test: 0.573)'
    ),
    'options_list': [
        'A. To increase the number of classes the model can handle during inference.',
        'B. To ensure the model only learns from classes that will appear during testing.',
        'C. To align the training process with the testing scenario, improving generalization to new tasks.',
        'D. To reduce computational complexity by limiting the number of classes considered.',
        'E. To enable the model to ignore classes from the base dataset during fine-tuning.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Simulating n-way tasks during meta-training aligns the training process with the testing scenario, which involves making '
        'predictions on new tasks with n classes and few examples per class. This alignment improves the model\'s ability to generalize '
        'to new tasks.'
    ),
    'chapter_information': 'Gatech Lectures - Few-Shot Learning'
}

question_Gatech_Lecture_FewShot_5 = {
    'question': (
        'In the context of few-shot learning, the prototypical network approach differs from the matching network approach primarily because it: (Probability of appearing on test: 0.956)'
    ),
    'options_list': [
        'A. Uses a learned metric instead of a hand-coded similarity function.',
        'B. Compares query items to mean embeddings of each class rather than individual support examples.',
        'C. Employs an LSTM-based meta-learner to update the feature extractor.',
        'D. Requires a larger support set per class to function effectively.',
        'E. Utilizes a black-box optimization method instead of gradient-based updates.'
    ],
    'correct_answer': 'B',
    'explanation': (
        'Prototypical networks compute mean embeddings (prototypes) for each class and compare query items to these prototypes, '
        'unlike matching networks that compare query items to individual support examples.'
    ),
    'chapter_information': 'Gatech Lectures - Few-Shot Learning'
}

question_Gatech_Lecture_FewShot_6 = {
    'question': (
        'True or False: One of the key benefits of model-agnostic meta-learning (MAML) is that it modifies the standard gradient descent update rule to include adaptive learning rates and weight decay, leading to better performance on few-shot learning tasks. (Probability of appearing on test: 0.879)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'MAML does not modify the standard gradient descent update rule. Instead, it focuses on learning a good initialization '
        'for the model parameters, such that applying standard gradient descent with a small amount of data leads to good performance on new tasks.'
    ),
    'chapter_information': 'Gatech Lectures - Few-Shot Learning'
}

question_Gatech_Lecture_FewShot_7 = {
    'question': (
        'Which of the following is a primary reason for backpropagating through the gradient descent steps in meta-learning algorithms like MAML? (Probability of appearing on test: 0.653)'
    ),
    'options_list': [
        'A. To learn a new optimizer that replaces gradient descent entirely.',
        'B. To adjust the learning rate schedule for faster convergence during training.',
        'C. To update the initialization parameters so that minimal fine-tuning is needed on new tasks.',
        'D. To incorporate unlabeled data into the training process through self-supervised learning.',
        'E. To enable the model to handle larger support sets without overfitting.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'By backpropagating through the gradient descent steps, meta-learning algorithms like MAML adjust the initialization parameters '
        'so that minimal fine-tuning (few gradient steps) is needed to achieve good performance on new tasks.'
    ),
    'chapter_information': 'Gatech Lectures - Few-Shot Learning'
}

question_Gatech_Lecture_FewShot_8 = {
    'question': (
        'In few-shot learning, which statement best captures the relationship between the support set and the query set during meta-training? (Probability of appearing on test: 0.783)'
    ),
    'options_list': [
        'A. The support set contains unlabeled data used to fine-tune the model, while the query set contains labeled data for evaluation.',
        'B. Both the support set and query set are subsets of the base dataset and contain overlapping classes.',
        'C. The support set is used to learn feature embeddings, and the query set is used to update the feature extractor.',
        'D. The support set provides labeled examples for new classes, and the query set is used to assess the model\'s ability to generalize to these classes.',
        'E. The support set and query set are merged to increase the number of examples per class for training.'
    ],
    'correct_answer': 'D',
    'explanation': (
        'In meta-training, the support set provides labeled examples for new classes, and the query set is used to assess the model\'s '
        'ability to generalize to these classes, simulating the few-shot learning scenario.'
    ),
    'chapter_information': 'Gatech Lectures - Few-Shot Learning'
}

question_Gatech_Lecture_FewShot_9 = {
    'question': (
        'True or False: Meta-learning approaches in few-shot learning exclusively focus on improving the feature extractor to prevent overfitting on small support sets, without modifying the classifier component. (Probability of appearing on test: 0.673)'
    ),
    'options_list': [
        'True',
        'False'
    ],
    'correct_answer': 'False',
    'explanation': (
        'Meta-learning approaches often modify both the feature extractor and the classifier component. For example, methods like MAML '
        'learn an initialization for the entire model (feature extractor and classifier), and some approaches focus on learning adaptive classifiers suitable for few-shot tasks.'
    ),
    'chapter_information': 'Gatech Lectures - Few-Shot Learning'
}

question_Gatech_Lecture_SelfSupervised_1 = {
    'question': (
        'Which of the following statements best describes the key difference between unsupervised learning and self-supervised learning? (Probability of appearing on test: 0.084)'
    ),
    'options_list': [
        'A. Unsupervised learning uses labeled data, while self-supervised learning uses unlabeled data.',
        'B. In unsupervised learning, the model learns to predict labels, whereas in self-supervised learning, the model learns to reconstruct its input.',
        'C. Self-supervised learning involves creating surrogate tasks where the labels are derived from the data itself, while unsupervised learning does not rely on any labels.',
        'D. Unsupervised learning is a subset of self-supervised learning.',
        'E. Self-supervised learning requires small amounts of labeled data, while unsupervised learning requires large amounts of unlabeled data.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Self-supervised learning involves creating surrogate tasks where labels are derived from the data itself (e.g., predicting rotation, colorization). '
        'Unsupervised learning, on the other hand, does not use any labels, including those generated from the data.'
    ),
    'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
}

question_Gatech_Lecture_SemiSupervised_1 = {
    'question': (
        'In semi-supervised learning, what type of data is typically used to improve model performance? (Probability of appearing on test: 0.634)'
    ),
    'options_list': [
        'A. Only large amounts of labeled data.',
        'B. Only small amounts of unlabeled data.',
        'C. A combination of a small amount of labeled data and a large amount of unlabeled data.',
        'D. Only data with pseudo-labels generated by clustering algorithms.',
        'E. Data augmented by synthetic labels generated through data augmentation.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Semi-supervised learning leverages a small amount of labeled data along with a large amount of unlabeled data to improve model performance.'
    ),
    'chapter_information': 'Gatech Lectures - Semi-Supervised Learning'
}

question_Gatech_Lecture_SelfSupervised_2 = {
    'question': (
        'Which of the following is an example of a self-supervised learning task? (Probability of appearing on test: 0.662)'
    ),
    'options_list': [
        'A. Training a classifier using labeled images to predict object categories.',
        'B. Using K-Means clustering to group unlabeled data into clusters.',
        'C. Predicting the rotation angle applied to an image as an output.',
        'D. Using principal component analysis to reduce the dimensionality of the data.',
        'E. Fine-tuning a pre-trained model on a new task with few labeled examples.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Predicting the rotation angle applied to an image is a self-supervised task where labels are derived from the data transformations.'
    ),
    'chapter_information': 'Gatech Lectures - Self-Supervised Learning'
}

question_Gatech_Lecture_Unsupervised_1 = {
    'question': (
        'In the context of autoencoders used in unsupervised learning, which of the following statements is true? (Probability of appearing on test: 0.791)'
    ),
    'options_list': [
        'A. Autoencoders require labeled data to learn effective feature representations.',
        'B. The loss function used in autoencoders is typically cross-entropy loss.',
        'C. Autoencoders learn to reconstruct the input data by encoding it into a lower-dimensional space and then decoding it back.',
        'D. Autoencoders are used to cluster data into meaningful groups.',
        'E. Autoencoders cannot be used to pre-train models for downstream tasks.'
    ],
    'correct_answer': 'C',
    'explanation': (
        'Autoencoders compress input data into a lower-dimensional representation (encoding) and then reconstruct the original data (decoding).'
    ),
    'chapter_information': 'Gatech Lectures - Unsupervised Learning'
}






KC_MPC_QUESTIONS = []
global_items = list(globals().items())
# print(global_items)

for name, value in global_items:
    if not name.startswith('_'):
        KC_MPC_QUESTIONS.append(value)

WEEK_18_MPC = KC_MPC_QUESTIONS[:-1]
