{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d99fe73-f5e5-4d36-959c-15ed4f239f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 14:11:27.807377: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-22 14:11:27.820634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732255887.833214   10152 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732255887.836660   10152 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 14:11:27.848743: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from data.bert_embeddings import BertFeatureExtractor\n",
    "from data.real_questions import REAL_QUESTIONS\n",
    "from CS_7643_Deep_Learning.dl_week11_mpc import WEEK_11_MPC\n",
    "from CS_7643_Deep_Learning.dl_week8_mpc import WEEK_8_MPC\n",
    "from CS_7643_Deep_Learning.dl_week5_mpc import WEEK_5_MPC\n",
    "from CS_7643_Deep_Learning.dl_week14_mpc import WEEK_14_MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16559f74-c795-446a-94c3-58946b6152af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(REAL_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b81be8-1293-44b1-8961-b3fb878673b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af56836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=0\n",
    "q=5-p\n",
    "coef = math.comb(5,5) \n",
    "success = (0.9)**p\n",
    "fail = (0.1)**1\n",
    "np.round(coef* success*fail ,4)*(p**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82110ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson(rate, x):\n",
    "       \n",
    "    if x==0:\n",
    "        return np.exp(-rate)\n",
    "    \n",
    "    return (np.exp(-rate)*rate**x)/math.factorial(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "786d7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greater(rate,x):\n",
    "    total = 0\n",
    "    for i in range(x+1):\n",
    "        total+=poisson(rate,i)\n",
    "    return 1-total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb31e2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14287653950145285"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greater(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf63c8b-594e-4449-a098-9df78df68a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_questions = []\n",
    "false_list = [WEEK_11_MPC, WEEK_8_MPC, WEEK_5_MPC, WEEK_14_MPC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48869062-852d-48a9-8914-23f00d880637",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_questions = []\n",
    "false_list = [WEEK_11_MPC, WEEK_8_MPC, WEEK_5_MPC, WEEK_14_MPC]\n",
    "for l in false_list:\n",
    "    for question in l:\n",
    "        \n",
    "        false = question['question'] + str(question['options_list'])\n",
    "        false_questions.append(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e68824-820c-4914-9664-213ad7dcb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_questions = []\n",
    "i=0\n",
    "for question in REAL_QUESTIONS:\n",
    "    # print(i)\n",
    "    # i+=1\n",
    "    truth = question['question'] + str(question['options_list'])\n",
    "    true_questions.append(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbaac879-9ae0-43c8-8f92-07cf7249bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame({'essay':false_questions})\n",
    "fake_df = pd.DataFrame({'essay':true_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372ab836-e932-459a-8caf-712d51fd944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = true_df.sample(100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a7af450-a611-4c7d-8d06-8df9ec6ee527",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b3f6cc0-d1c1-4551-97d9-4c8052069b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df['label'] = 0\n",
    "fake_df['label'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f985a8bf-5c1f-498a-9599-7110e738618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([true_df,fake_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c91d20f-fa16-48eb-93ef-5d9602a3cbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    " extractor = BertFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c395633a-88da-4707-9b12-ace5488a936b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57722efec7be4055a52997ad953119d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = extractor.transform(df,embedding_type='pooler_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a32a6c0-f4df-423b-af18-dd7b12b288cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8571428571428571\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87        20\n",
      "           1       0.81      0.87      0.84        15\n",
      "\n",
      "    accuracy                           0.86        35\n",
      "   macro avg       0.85      0.86      0.86        35\n",
      "weighted avg       0.86      0.86      0.86        35\n",
      "\n",
      "Test Accuracy: 0.8571428571428571\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        20\n",
      "           1       0.86      0.80      0.83        15\n",
      "\n",
      "    accuracy                           0.86        35\n",
      "   macro avg       0.86      0.85      0.85        35\n",
      "weighted avg       0.86      0.86      0.86        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Combine the embeddings and labels into a single DataFrame\n",
    "df['embeddings'] = list(embeddings)\n",
    "X = list(df['embeddings'])\n",
    "y = df['label']\n",
    "\n",
    "# Split into training + validation and test sets (80% train+val, 20% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split training + validation set into training and validation sets (75% train, 25% val from train+val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Convert lists of embeddings to numpy arrays for compatibility with SVM\n",
    "import numpy as np\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Train an SVM with RBF kernel\n",
    "svm_model = SVC(kernel='rbf', C=95.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb93db2c-5b5c-4007-9bc4-7c5413a64652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7428571428571429\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78        20\n",
      "           1       0.71      0.67      0.69        15\n",
      "\n",
      "    accuracy                           0.74        35\n",
      "   macro avg       0.74      0.73      0.74        35\n",
      "weighted avg       0.74      0.74      0.74        35\n",
      "\n",
      "Test Accuracy: 0.8571428571428571\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        20\n",
      "           1       0.86      0.80      0.83        15\n",
      "\n",
      "    accuracy                           0.86        35\n",
      "   macro avg       0.86      0.85      0.85        35\n",
      "weighted avg       0.86      0.86      0.86        35\n",
      "\n",
      "Predicted probabilities for the first 5 validation samples:\n",
      "[[0.65053171 0.34946829]\n",
      " [0.51590383 0.48409617]\n",
      " [0.4480176  0.5519824 ]\n",
      " [0.77285231 0.22714769]\n",
      " [0.33855592 0.66144408]]\n",
      "Validation ROC-AUC Score: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "# Combine the embeddings and labels into a single DataFrame\n",
    "df['embeddings'] = list(embeddings)\n",
    "X = list(df['embeddings'])\n",
    "y = df['label']\n",
    "\n",
    "# Split into training + validation and test sets (80% train+val, 20% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split training + validation set into training and validation sets (75% train, 25% val from train+val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Convert lists of embeddings to numpy arrays for compatibility with SVM\n",
    "import numpy as np\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Train an SVM with RBF kernel and probability predictions enabled\n",
    "svm_model = SVC(kernel='rbf', C=45.0, gamma='scale', probability=True, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "y_val_probs = svm_model.predict_proba(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set using hard labels\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate the model on the test set using probabilities\n",
    "y_test_probs = svm_model.predict_proba(X_test)\n",
    "y_test_pred = svm_model.predict(X_test)  # Hard labels\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Example: Print probabilities for the first 5 validation samples\n",
    "print(\"Predicted probabilities for the first 5 validation samples:\")\n",
    "print(y_val_probs[:5])\n",
    "\n",
    "# If evaluating using probabilities, calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_val, y_val_probs[:, 1])  # Use probabilities for the positive class\n",
    "print(\"Validation ROC-AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7698e1b9-ad71-49cb-a955-3349e4b1489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CS_7643_Deep_Learning.dl_week17_mpc import WEEK_17_MPC\n",
    "from CS_7643_Deep_Learning.dl_week18_mpc import WEEK_18_MPC\n",
    "from CS_7643_Deep_Learning.dl_week19_mpc import WEEK_19_MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9486f569-f35c-408a-987b-c5951a25bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_questions = []\n",
    "# for question_list in [ WEEK_17_MPC, WEEK_18_MPC, WEEK_19_MPC]:\n",
    "#     for question in ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceed02a4-5ad5-4456-8964-359f68edd553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to predict probabilities and update questions\n",
    "def add_svm_probability_to_questions(questions, svm_model, extractor):\n",
    "    # Combine question text and options into a single list\n",
    "    question_texts = [q['question'] + \" \" + \" \".join(q['options_list']) for q in questions]\n",
    "\n",
    "    # Convert question texts to a DataFrame\n",
    "    df = pd.DataFrame({'essay': question_texts})\n",
    "\n",
    "    # Extract embeddings for all questions using BERT\n",
    "    embeddings = extractor.transform(df, embedding_type='pooler_output')\n",
    "\n",
    "    # Ensure embeddings are in the correct format (numpy array)\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    # Predict probabilities for class 1\n",
    "    probabilities = svm_model.predict_proba(embeddings)[:, 1]\n",
    "\n",
    "    # Update each question dictionary with the probability\n",
    "    for i, question_dict in enumerate(questions):\n",
    "        prob_class_1 = probabilities[i]\n",
    "        updated_question = f\"{question_dict['question']} (Probability of appearing on test: {prob_class_1:.3f})\"\n",
    "        question_dict['question'] = updated_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "088aef1d-6c8d-4028-ae7e-7df5ea4fa7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccd05659fdf4f6b8a992129fac9959d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which of the following is TRUE about GANs and VAEs? (Probability of appearing on test: 0.745) (Probability of appearing on test: 0.729)\n",
      "Which of the following best describes the role of the discriminator in a Generative Adversarial Network (GAN)? (Probability of appearing on test: 0.743) (Probability of appearing on test: 0.776)\n",
      "Which of the following loss functions is used to train the generator in a standard GAN? (Probability of appearing on test: 0.966) (Probability of appearing on test: 0.971)\n",
      "Which of the following statements is true about the training process of Variational Autoencoders (VAEs)? (Probability of appearing on test: 0.934) (Probability of appearing on test: 0.945)\n",
      "How do GANs differ from VAEs in their approach to generative modeling? (Probability of appearing on test: 0.779) (Probability of appearing on test: 0.764)\n",
      "What is a key limitation of GANs compared to VAEs? (Probability of appearing on test: 0.949) (Probability of appearing on test: 0.977)\n",
      "True or False: GANs model an explicit probability density function over the data distribution. (Probability of appearing on test: 0.902) (Probability of appearing on test: 0.888)\n",
      "True or False: In a VAE, the latent space is regularized to follow a prior distribution, such as a standard Gaussian. (Probability of appearing on test: 0.862) (Probability of appearing on test: 0.855)\n",
      "True or False: The generator in a GAN directly optimizes the KL-divergence between the generated and true distributions. (Probability of appearing on test: 0.775) (Probability of appearing on test: 0.658)\n",
      "True or False: VAEs are capable of both generating new samples and reconstructing inputs using their encoder-decoder structure. (Probability of appearing on test: 0.850) (Probability of appearing on test: 0.748)\n",
      "True or False: GANs can suffer from mode collapse, where the generator only produces a limited variety of outputs. (Probability of appearing on test: 0.836) (Probability of appearing on test: 0.849)\n",
      "What is the main difference between Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)? (Probability of appearing on test: 0.846) (Probability of appearing on test: 0.827)\n",
      "In GANs, what is the role of the discriminator? (Probability of appearing on test: 0.462) (Probability of appearing on test: 0.740)\n",
      "What is the primary objective of the generator in GANs? (Probability of appearing on test: 0.916) (Probability of appearing on test: 0.956)\n",
      "What loss function does the generator minimize in the modified GAN training objective? (Probability of appearing on test: 0.848) (Probability of appearing on test: 0.900)\n",
      "What is \"mode collapse\" in GANs? (Probability of appearing on test: 0.535) (Probability of appearing on test: 0.812)\n",
      "True or False: GANs explicitly learn a probability density function $p(x)$. (Probability of appearing on test: 0.890) (Probability of appearing on test: 0.861)\n",
      "True or False: Variational Autoencoders (VAEs) optimize a reconstruction loss and a regularization term that enforces a Gaussian distribution in the latent space. (Probability of appearing on test: 0.970) (Probability of appearing on test: 0.910)\n",
      "True or False: GANs rely on two neural networks: a generator and a discriminator, which compete in a minimax optimization game. (Probability of appearing on test: 0.790) (Probability of appearing on test: 0.786)\n",
      "True or False: Adding noise to real images during GAN training can improve stability and reduce mode collapse. (Probability of appearing on test: 0.881) (Probability of appearing on test: 0.854)\n",
      "True or False: The generator in a GAN can function independently of the discriminator once training is complete. (Probability of appearing on test: 0.819) (Probability of appearing on test: 0.874)\n",
      "In a Variational Autoencoder (VAE), which of the following statements best describes the role of the encoder and decoder networks? (Probability of appearing on test: 0.368) (Probability of appearing on test: 0.348)\n",
      "What is the primary objective function used to train a VAE? (Probability of appearing on test: 0.807) (Probability of appearing on test: 0.917)\n",
      "In training a VAE, the Kullback-Leibler (KL) divergence term in the loss function serves what purpose? (Probability of appearing on test: 0.500) (Probability of appearing on test: 0.428)\n",
      "In a Generative Adversarial Network (GAN), what are the roles of the generator and discriminator? (Probability of appearing on test: 0.500) (Probability of appearing on test: 0.557)\n",
      "Which of the following best describes the training objective of a GAN? (Probability of appearing on test: 0.548) (Probability of appearing on test: 0.649)\n",
      "What is a key difference between VAEs and GANs in terms of their approach to generative modeling? (Probability of appearing on test: 0.836) (Probability of appearing on test: 0.893)\n",
      "In the context of VAEs, what is the \"reparameterization trick\"? (Probability of appearing on test: 0.712) (Probability of appearing on test: 0.853)\n",
      "In GANs, which of the following is a common problem that arises during training? (Probability of appearing on test: 0.841) (Probability of appearing on test: 0.904)\n",
      "Which loss function is typically minimized by the discriminator in a standard GAN? (Probability of appearing on test: 0.944) (Probability of appearing on test: 0.934)\n",
      "True or False: VAEs generally produce sharper images than GANs because they maximize the data likelihood directly. (Probability of appearing on test: 0.877) (Probability of appearing on test: 0.891)\n",
      "In the context of GANs, what is the primary role of the generator network? (Probability of appearing on test: 0.470) (Probability of appearing on test: 0.529)\n",
      "What does the discriminator network aim to achieve in a GAN? (Probability of appearing on test: 0.671) (Probability of appearing on test: 0.755)\n",
      "Which of the following best describes the training objective of a standard GAN? (Probability of appearing on test: 0.798) (Probability of appearing on test: 0.822)\n",
      "What is mode collapse in the context of GANs? (Probability of appearing on test: 0.581) (Probability of appearing on test: 0.618)\n",
      "Which of the following is a common technique to mitigate mode collapse in GANs? (Probability of appearing on test: 0.633) (Probability of appearing on test: 0.746)\n",
      "True or False: GANs always minimize the Kullback-Leibler divergence between the data distribution and the model distribution. (Probability of appearing on test: 0.909) (Probability of appearing on test: 0.924)\n",
      "True or False: In GANs, the generator network directly accesses the training data during the update. (Probability of appearing on test: 0.585) (Probability of appearing on test: 0.659)\n",
      "True or False: Batch normalization is used in GANs to stabilize training by normalizing inputs to each layer. (Probability of appearing on test: 0.879) (Probability of appearing on test: 0.871)\n",
      "True or False: Mode collapse occurs when the discriminator fails to learn and the generator overfits the training data. (Probability of appearing on test: 0.814) (Probability of appearing on test: 0.871)\n",
      "True or False: GANs can be used for semi-supervised learning by modifying the discriminator to output class labels. (Probability of appearing on test: 0.768) (Probability of appearing on test: 0.813)\n",
      "Which of the following statements best describes the role of the reparameterization trick in Variational Autoencoders (VAEs)? (Probability of appearing on test: 0.931) (Probability of appearing on test: 0.859)\n",
      "What is the primary purpose of the KL divergence term $D_{KL}(Q(z\\mid X) \\parallel P(z))$ in the VAE loss function? (Probability of appearing on test: 0.974) (Probability of appearing on test: 0.981)\n",
      "When training a Variational Autoencoder, which of the following challenges is directly addressed by the use of a probabilistic decoder $P(X\\mid z)$? (Probability of appearing on test: 0.873) (Probability of appearing on test: 0.912)\n",
      "Which of the following conditions would most likely result in poor performance of a VAE? (Probability of appearing on test: 0.923) (Probability of appearing on test: 0.913)\n",
      "Which of the following methods extends VAEs for tasks involving conditional generation (e.g., image completion)? (Probability of appearing on test: 0.772) (Probability of appearing on test: 0.763)\n",
      "True or False: The encoder in a Variational Autoencoder is trained to output deterministic latent variables $z$. (Probability of appearing on test: 0.822) (Probability of appearing on test: 0.732)\n",
      "True or False: The reconstruction loss in a VAE objective function is always computed using the $\\ell_2$-norm between the input $X$ and its reconstruction. (Probability of appearing on test: 0.559) (Probability of appearing on test: 0.736)\n",
      "True or False: VAEs assume that the posterior $Q(z\\mid X)$ is independent of the prior $P(z)$. (Probability of appearing on test: 0.883) (Probability of appearing on test: 0.791)\n",
      "True or False: Sampling $z \\sim N(\\mu, \\sigma^2)$ directly during training makes the VAE training objective non-differentiable. (Probability of appearing on test: 0.755) (Probability of appearing on test: 0.729)\n",
      "True or False: Increasing the dimensionality of the latent space always improves the generative quality of a VAE. (Probability of appearing on test: 0.881) (Probability of appearing on test: 0.908)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update questions with probabilities\n",
    "add_svm_probability_to_questions(questions=WEEK_19_MPC, svm_model=svm_model, extractor=extractor)\n",
    "\n",
    "# Print updated questions to verify\n",
    "for q in WEEK_19_MPC:\n",
    "    print(q['question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb638e50-72a4-4a2d-be68-65f0c9a373f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated questions saved to updated_week_17_mpc.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \"updated_week_17_mpc.json\"\n",
    "\n",
    "# Write the updated questions to a JSON file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(WEEK_17_MPC, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Updated questions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83373554-c632-48c3-ba37-f754de8afe31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
